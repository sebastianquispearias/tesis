{
    "sourceFile": "main_semisup_pytorch_enhancer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 9,
            "patches": [
                {
                    "date": 1750570942975,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1750610674230,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -177,10 +177,29 @@\n \r\n     # Optimizador y pérdidas\r\n     optimizer = optim.Adam(student.parameters(), lr=LR)\r\n     sup_loss   = smp.losses.DiceLoss(mode='binary')\r\n-    cons_loss  = nn.MSELoss()\r\n+    # --- TU APORTE: DISEÑO DE LA LIBRERÍA DE KERNELS BASADO EN TU ANÁLISIS ---\r\n+    diametro_p25 = 18\r\n+    diametro_p75 = 76\r\n+    aspect_ratio_mediano = 2.34\r\n \r\n+    # Crear la librería de kernels\r\n+    kernel_pequeno_hor = crear_kernel_elipse(diametro_p25, aspect_ratio_mediano, angulo_grados=0, device=device)\r\n+    kernel_pequeno_ver = crear_kernel_elipse(diametro_p25, aspect_ratio_mediano, angulo_grados=90, device=device)\r\n+    kernel_grande_hor = crear_kernel_elipse(diametro_p75, aspect_ratio_mediano, angulo_grados=0, device=device)\r\n+    kernel_grande_ver = crear_kernel_elipse(diametro_p75, aspect_ratio_mediano, angulo_grados=90, device=device)\r\n+\r\n+    libreria_de_kernels = [kernel_pequeno_hor, kernel_pequeno_ver, kernel_grande_hor, kernel_grande_ver]\r\n+\r\n+    # Instancia tu loss morfológico compuesto\r\n+    loss_morfologico = LossConsistenciaMorfologicaCompuesta(lista_kernels=libreria_de_kernels)\r\n+\r\n+    # El resto de tu bucle de entrenamiento en train.py permanece igual.\r\n+    # Solo asegúrate de llamar a tu nuevo loss:\r\n+    L_morf = loss_morfologico(torch.sigmoid(pred_student_no_sup), torch.sigmoid(pred_teacher_no_sup))    \r\n+    #cons_loss  = nn.MSELoss()\r\n+    cons_loss  = L_morf\r\n     best_iou = 0.0\r\n     for epoch in range(1, EPOCHS+1):\r\n         student.train()\r\n         epoch_sup, epoch_cons = 0.0, 0.0\r\n"
                },
                {
                    "date": 1750610881710,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -19,9 +19,10 @@\n \r\n import segmentation_models_pytorch as smp\r\n import albumentations as A\r\n from albumentations.pytorch import ToTensorV2\r\n-\r\n+from utils import crear_kernel_elipse, actualizar_teacher_ema\r\n+from losses import LossConsistenciaMorfologicaCompuesta\r\n # ─────────── 0) Parámetros globales ───────────\r\n SEED          = 42\r\n BATCH_SIZE    = 2\r\n LR            = 1e-4\r\n"
                },
                {
                    "date": 1750611224907,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -129,9 +129,9 @@\n     args = parse_args()\r\n     regime = args.regime\r\n     # Logging a archivo\r\n     logging.basicConfig(level=logging.INFO,\r\n-                        handlers=[logging.FileHandler(\"entrenamiento_log_pytorch.txt\"),\r\n+                        handlers=[logging.FileHandler(\"entrenamiento_log_pytorch_enhancer.txt\"),\r\n                                   logging.StreamHandler(sys.stdout)],\r\n                         format=\"%(asctime)s %(levelname)s: %(message)s\")\r\n     logging.info(f\"Dispositivo: {DEVICE}\")\r\n \r\n@@ -192,15 +192,11 @@\n \r\n     libreria_de_kernels = [kernel_pequeno_hor, kernel_pequeno_ver, kernel_grande_hor, kernel_grande_ver]\r\n \r\n     # Instancia tu loss morfológico compuesto\r\n-    loss_morfologico = LossConsistenciaMorfologicaCompuesta(lista_kernels=libreria_de_kernels)\r\n+    cons_loss  = LossConsistenciaMorfologicaCompuesta(lista_kernels=libreria_de_kernels)\r\n+    logging.info(f\"Usando Loss de Consistencia Morfológica Compuesta con {len(libreria_de_kernels)} kernels.\")\r\n \r\n-    # El resto de tu bucle de entrenamiento en train.py permanece igual.\r\n-    # Solo asegúrate de llamar a tu nuevo loss:\r\n-    L_morf = loss_morfologico(torch.sigmoid(pred_student_no_sup), torch.sigmoid(pred_teacher_no_sup))    \r\n-    #cons_loss  = nn.MSELoss()\r\n-    cons_loss  = L_morf\r\n     best_iou = 0.0\r\n     for epoch in range(1, EPOCHS+1):\r\n         student.train()\r\n         epoch_sup, epoch_cons = 0.0, 0.0\r\n"
                },
                {
                    "date": 1750611418857,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -199,10 +199,11 @@\n     best_iou = 0.0\r\n     for epoch in range(1, EPOCHS+1):\r\n         student.train()\r\n         epoch_sup, epoch_cons = 0.0, 0.0\r\n- \r\n-        for (x_s, y_s), (x_us, _), (x_uw, _) in zip(sup_loader, unlab_strong_loader, unlab_weak_loader):\r\n+        loop = tqdm(zip(sup_loader, unlab_strong_loader, unlab_weak_loader), total=len(sup_loader), leave=False)\r\n+\r\n+        for (x_s, y_s), (x_us, _), (x_uw, _) in loop:\r\n             x_s, y_s = x_s.to(DEVICE), y_s.to(DEVICE)\r\n             x_us = x_us.to(DEVICE)   # student on strong\r\n             x_uw = x_uw.to(DEVICE)   # teacher on weak\r\n \r\n@@ -230,9 +231,10 @@\n                 pt.data.mul_(EMA_ALPHA).add_(ps.data * (1-EMA_ALPHA))\r\n \r\n             epoch_sup  += loss_s.item()\r\n             epoch_cons += loss_c.item()\r\n-\r\n+            loop.set_description(f\"Epoch [{epoch}/{EPOCHS}]\")\r\n+            loop.set_postfix(loss_sup=loss_s.item(), loss_cons=loss_c.item())\r\n         # ——— Validación (pérdida + métricas) ———\r\n         student.eval()\r\n         epoch_val_loss = 0.0\r\n         iou, prec, rec, f1 = 0.0, 0.0, 0.0, 0.0\r\n"
                },
                {
                    "date": 1750611852279,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -184,12 +184,12 @@\n     diametro_p75 = 76\r\n     aspect_ratio_mediano = 2.34\r\n \r\n     # Crear la librería de kernels\r\n-    kernel_pequeno_hor = crear_kernel_elipse(diametro_p25, aspect_ratio_mediano, angulo_grados=0, device=device)\r\n-    kernel_pequeno_ver = crear_kernel_elipse(diametro_p25, aspect_ratio_mediano, angulo_grados=90, device=device)\r\n-    kernel_grande_hor = crear_kernel_elipse(diametro_p75, aspect_ratio_mediano, angulo_grados=0, device=device)\r\n-    kernel_grande_ver = crear_kernel_elipse(diametro_p75, aspect_ratio_mediano, angulo_grados=90, device=device)\r\n+    kernel_pequeno_hor = crear_kernel_elipse(diametro_p25, aspect_ratio_mediano, angulo_grados=0, device=DEVICE)\r\n+    kernel_pequeno_ver = crear_kernel_elipse(diametro_p25, aspect_ratio_mediano, angulo_grados=90, device=DEVICE)\r\n+    kernel_grande_hor = crear_kernel_elipse(diametro_p75, aspect_ratio_mediano, angulo_grados=0, device=DEVICE)\r\n+    kernel_grande_ver = crear_kernel_elipse(diametro_p75, aspect_ratio_mediano, angulo_grados=90, device=DEVICE)\r\n \r\n     libreria_de_kernels = [kernel_pequeno_hor, kernel_pequeno_ver, kernel_grande_hor, kernel_grande_ver]\r\n \r\n     # Instancia tu loss morfológico compuesto\r\n"
                },
                {
                    "date": 1750645171910,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -36,9 +36,9 @@\n # Mean Teacher\r\n EMA_ALPHA     = 0.99\r\n CONS_MAX      = 1.0\r\n CONS_RAMPUP   = 30\r\n-UNLABELED_W   = 1.0\r\n+UNLABELED_W   = 0.1\r\n \r\n def get_consistency_weight(epoch):\r\n     if epoch >= CONS_RAMPUP:\r\n         return CONS_MAX\r\n"
                },
                {
                    "date": 1750645652892,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -179,22 +179,18 @@\n     # Optimizador y pérdidas\r\n     optimizer = optim.Adam(student.parameters(), lr=LR)\r\n     sup_loss   = smp.losses.DiceLoss(mode='binary')\r\n     # --- TU APORTE: DISEÑO DE LA LIBRERÍA DE KERNELS BASADO EN TU ANÁLISIS ---\r\n-    diametro_p25 = 18\r\n-    diametro_p75 = 76\r\n+    # Usaremos solo un kernel de tamaño mediano y forma elíptica\r\n+    diametro_mediano = 32\r\n     aspect_ratio_mediano = 2.34\r\n \r\n-    # Crear la librería de kernels\r\n-    kernel_pequeno_hor = crear_kernel_elipse(diametro_p25, aspect_ratio_mediano, angulo_grados=0, device=DEVICE)\r\n-    kernel_pequeno_ver = crear_kernel_elipse(diametro_p25, aspect_ratio_mediano, angulo_grados=90, device=DEVICE)\r\n-    kernel_grande_hor = crear_kernel_elipse(diametro_p75, aspect_ratio_mediano, angulo_grados=0, device=DEVICE)\r\n-    kernel_grande_ver = crear_kernel_elipse(diametro_p75, aspect_ratio_mediano, angulo_grados=90, device=DEVICE)\r\n+    # Creamos un único kernel, por ejemplo, horizontal\r\n+    kernel_unico = crear_kernel_elipse(diametro_mediano, aspect_ratio_mediano, angulo_grados=0, device=DEVICE)\r\n \r\n-    libreria_de_kernels = [kernel_pequeno_hor, kernel_pequeno_ver, kernel_grande_hor, kernel_grande_ver]\r\n+    libreria_de_kernels = [kernel_unico] # La librería ahora solo tiene un elemento\r\n \r\n-    # Instancia tu loss morfológico compuesto\r\n-    cons_loss  = LossConsistenciaMorfologicaCompuesta(lista_kernels=libreria_de_kernels)\r\n+    cons_loss = LossConsistenciaMorfologicaCompuesta(lista_kernels=libreria_de_kernels)\r\n     logging.info(f\"Usando Loss de Consistencia Morfológica Compuesta con {len(libreria_de_kernels)} kernels.\")\r\n \r\n     best_iou = 0.0\r\n     for epoch in range(1, EPOCHS+1):\r\n"
                },
                {
                    "date": 1750677962353,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,306 @@\n+# main_semisup_pytorch.py\r\n+\r\n+import os\r\n+os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\r\n+\r\n+import sys\r\n+import argparse\r\n+import random\r\n+import logging\r\n+import numpy as np\r\n+from tqdm import tqdm\r\n+\r\n+import torch\r\n+import torch.nn as nn\r\n+import torch.optim as optim\r\n+from torch.utils.data import DataLoader, Dataset\r\n+from torchvision import transforms\r\n+from PIL import Image\r\n+\r\n+import segmentation_models_pytorch as smp\r\n+import albumentations as A\r\n+from albumentations.pytorch import ToTensorV2\r\n+from utils import crear_kernel_elipse, actualizar_teacher_ema\r\n+from losses import LossConsistenciaMorfologicaCompuesta\r\n+# ─────────── 0) Parámetros globales ───────────\r\n+SEED          = 42\r\n+BATCH_SIZE    = 2\r\n+LR            = 1e-4\r\n+EPOCHS        = 80\r\n+INPUT_SHAPE   = (384, 384)\r\n+DATA_ROOT     = r\"./data/all_results/regimes\"  # ajusta si hace falta\r\n+CLASSES       = [\"corrosion\"]\r\n+N_CLASSES     = 1\r\n+DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n+\r\n+# Mean Teacher\r\n+EMA_ALPHA     = 0.99\r\n+CONS_MAX      = 1.0\r\n+CONS_RAMPUP   = 30\r\n+UNLABELED_W   = 0.1\r\n+\r\n+def get_consistency_weight(epoch):\r\n+    if epoch >= CONS_RAMPUP:\r\n+        return CONS_MAX\r\n+    phase = 1.0 - epoch/CONS_RAMPUP\r\n+    return CONS_MAX * np.exp(-5 * phase * phase)  # :contentReference[oaicite:4]{index=4}\r\n+\r\n+# ─────────── 1) Argumentos ───────────\r\n+def parse_args():\r\n+    p = argparse.ArgumentParser()\r\n+    p.add_argument(\"--regime\", required=True,\r\n+                   choices=[\"C10\",\"C25\",\"C50\",\"C75\"],\r\n+                   help=\"Carpeta de régimen dentro de data/all_results/regimes\")\r\n+    return p.parse_args()\r\n+\r\n+# ─────────── 2) Dataset ───────────\r\n+class CorrosionDataset(Dataset):\r\n+    def __init__(self, images_dir, masks_dir, \r\n+                 transform=None, mode=\"both\"):\r\n+        self.images = sorted(os.listdir(images_dir))\r\n+        self.images_dir = images_dir\r\n+        self.masks_dir  = masks_dir\r\n+        self.transform  = transform\r\n+        self.mode       = mode  # \"labeled\", \"unlabeled\", \"both\"\r\n+\r\n+    def __len__(self):\r\n+        return len(self.images)\r\n+\r\n+    def __getitem__(self, idx):\r\n+        fname = self.images[idx]\r\n+        img = np.array(Image.open(os.path.join(self.images_dir, fname)).convert(\"RGB\"))\r\n+        if self.mode in (\"both\",\"labeled\"):\r\n+            mask = np.array(Image.open(os.path.join(self.masks_dir, fname)).convert(\"L\"))\r\n+            mask = (mask>0).astype(\"float32\")[...,None]\r\n+        else:\r\n+            mask = np.zeros((img.shape[0],img.shape[1],1),dtype=\"float32\")\r\n+\r\n+        if self.transform:\r\n+            augmented = self.transform(image=img, mask=mask)\r\n+            img = augmented[\"image\"]\r\n+            mask = augmented[\"mask\"]\r\n+            # --- aquí convertimos mask a [1, H, W] tensor ---\r\n+            if isinstance(mask, np.ndarray):\r\n+                mask = torch.from_numpy(mask).permute(2,0,1).float()\r\n+            else:\r\n+                # albumentations a veces devuelve Tensor con shape [H,W,1]\r\n+                if mask.ndim==3 and mask.shape[2]==1:\r\n+                    mask = mask.permute(2,0,1)\r\n+                elif mask.ndim==2:\r\n+                    mask = mask.unsqueeze(0)\r\n+        else:\r\n+            img = ToTensorV2()(image=img)[\"image\"]\r\n+            mask = torch.from_numpy(mask).permute(2,0,1).float()\r\n+\r\n+        return img, mask\r\n+\r\n+# ─────────── 3) Transforms ───────────\r\n+# ─────────── 3) Transforms ───────────\r\n+train_strong = A.Compose([\r\n+    A.Resize(*INPUT_SHAPE),\r\n+    A.HorizontalFlip(p=0.5),\r\n+    A.VerticalFlip(p=0.5),\r\n+    A.RandomRotate90(p=0.5),\r\n+    A.Transpose(p=0.5),\r\n+    A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\r\n+    A.RandomBrightnessContrast(p=0.5),\r\n+    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),\r\n+    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\r\n+    ToTensorV2(),\r\n+])\r\n+\r\n+train_weak = A.Compose([\r\n+    A.Resize(*INPUT_SHAPE),\r\n+    A.HorizontalFlip(p=0.2),\r\n+    A.RandomBrightnessContrast(p=0.1),\r\n+    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\r\n+    ToTensorV2(),\r\n+])\r\n+\r\n+val_transform = A.Compose([\r\n+    A.Resize(*INPUT_SHAPE),\r\n+    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\r\n+    ToTensorV2(),\r\n+])\r\n+\r\n+\r\n+# ─────────── 4) Main ───────────\r\n+def main():\r\n+    args = parse_args()\r\n+    regime = args.regime\r\n+    # Logging a archivo\r\n+    logging.basicConfig(level=logging.INFO,\r\n+                        handlers=[logging.FileHandler(\"entrenamiento_log_pytorch_enhancer.txt\"),\r\n+                                  logging.StreamHandler(sys.stdout)],\r\n+                        format=\"%(asctime)s %(levelname)s: %(message)s\")\r\n+    logging.info(f\"Dispositivo: {DEVICE}\")\r\n+\r\n+    # Rutas\r\n+    root = os.path.join(DATA_ROOT, regime)\r\n+    xl = os.path.join(root, \"train/images_labeled\")\r\n+    yl = os.path.join(root, \"train/masks_labeled\")\r\n+    xu = os.path.join(root, \"train/images_unlabeled\")\r\n+    xv = os.path.join(root, \"val/images\")\r\n+    yv = os.path.join(root, \"val/masks\")\r\n+\r\n+    # DataLoaders\r\n+    # DataLoaders\r\n+    sup_ds = CorrosionDataset(xl, yl, transform=train_strong, mode=\"labeled\")\r\n+    sup_loader = DataLoader(sup_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\r\n+\r\n+    unlab_strong_ds = CorrosionDataset(xu, xu, transform=train_strong, mode=\"unlabeled\")\r\n+    unlab_strong_loader = DataLoader(unlab_strong_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\r\n+\r\n+    unlab_weak_ds = CorrosionDataset(xu, xu, transform=train_weak, mode=\"unlabeled\")\r\n+    unlab_weak_loader = DataLoader(unlab_weak_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\r\n+\r\n+    val_ds = CorrosionDataset(xv, yv, transform=val_transform, mode=\"labeled\")\r\n+    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\r\n+\r\n+    # Modelos\r\n+    student = smp.DeepLabV3Plus(\r\n+        encoder_name=\"efficientnet-b3\",\r\n+        encoder_weights=\"imagenet\",\r\n+        in_channels=3,\r\n+        classes=N_CLASSES,\r\n+        activation=None,\r\n+    ).to(DEVICE)\r\n+\r\n+    teacher = smp.DeepLabV3Plus(\r\n+        encoder_name=\"efficientnet-b3\",\r\n+        encoder_weights=\"imagenet\",\r\n+        in_channels=3,\r\n+        classes=N_CLASSES,\r\n+        activation=None,\r\n+    ).to(DEVICE)\r\n+    teacher.load_state_dict(student.state_dict())\r\n+    for p in teacher.parameters(): p.requires_grad = False\r\n+\r\n+    # Optimizador y pérdidas\r\n+    optimizer = optim.Adam(student.parameters(), lr=LR)\r\n+    sup_loss   = smp.losses.DiceLoss(mode='binary')\r\n+    # --- TU APORTE: DISEÑO DE LA LIBRERÍA DE KERNELS BASADO EN TU ANÁLISIS ---\r\n+    # Usaremos solo un kernel de tamaño mediano y forma elíptica\r\n+    diametro_mediano = 32\r\n+    aspect_ratio_mediano = 2.34\r\n+\r\n+    # Creamos un único kernel, por ejemplo, horizontal\r\n+    kernel_unico = crear_kernel_elipse(diametro_mediano, aspect_ratio_mediano, angulo_grados=0, device=DEVICE)\r\n+\r\n+    libreria_de_kernels = [kernel_unico] # La librería ahora solo tiene un elemento\r\n+\r\n+    cons_loss = LossConsistenciaMorfologicaCompuesta(lista_kernels=libreria_de_kernels)\r\n+    logging.info(f\"Usando Loss de Consistencia Morfológica Compuesta con {len(libreria_de_kernels)} kernels.\")\r\n+\r\n+    best_iou = 0.0\r\n+    for epoch in range(1, EPOCHS+1):\r\n+        student.train()\r\n+        epoch_sup, epoch_cons = 0.0, 0.0\r\n+        loop = tqdm(zip(sup_loader, unlab_strong_loader, unlab_weak_loader), total=len(sup_loader), leave=False)\r\n+\r\n+        for (x_s, y_s), (x_us, _), (x_uw, _) in loop:\r\n+            x_s, y_s = x_s.to(DEVICE), y_s.to(DEVICE)\r\n+            x_us = x_us.to(DEVICE)   # student on strong\r\n+            x_uw = x_uw.to(DEVICE)   # teacher on weak\r\n+\r\n+            # Forward supervised\r\n+            log_s = student(x_s)\r\n+            loss_s = sup_loss(log_s, y_s)\r\n+\r\n+            # Forward unsupervised\r\n+            with torch.no_grad():\r\n+                t_uw = torch.sigmoid(teacher(x_uw))\r\n+            s_us = torch.sigmoid(student(x_us))\r\n+\r\n+            w = get_consistency_weight(epoch)\r\n+            loss_c = cons_loss(s_us, t_uw) * w * UNLABELED_W\r\n+\r\n+            loss = loss_s + loss_c\r\n+            optimizer.zero_grad()\r\n+            loss.backward()\r\n+            optimizer.step()\r\n+\r\n+            # EMA update...\r\n+\r\n+            # EMA update\r\n+            for ps, pt in zip(student.parameters(), teacher.parameters()):\r\n+                pt.data.mul_(EMA_ALPHA).add_(ps.data * (1-EMA_ALPHA))\r\n+\r\n+            epoch_sup  += loss_s.item()\r\n+            epoch_cons += loss_c.item()\r\n+            loop.set_description(f\"Epoch [{epoch}/{EPOCHS}]\")\r\n+            loop.set_postfix(loss_sup=loss_s.item(), loss_cons=loss_c.item())\r\n+        # ——— Validación (pérdida + métricas) ———\r\n+        student.eval()\r\n+        epoch_val_loss = 0.0\r\n+        iou, prec, rec, f1 = 0.0, 0.0, 0.0, 0.0\r\n+        n_val = 0\r\n+        for x_val, y_val in val_loader:\r\n+            x_val, y_val = x_val.to(DEVICE), y_val.to(DEVICE)\r\n+            with torch.no_grad():\r\n+                logits_val = student(x_val)\r\n+                # val_loss sobre BCEWithLogits\r\n+                loss_val = sup_loss(logits_val, y_val)\r\n+                epoch_val_loss += loss_val.item()\r\n+                pred = torch.sigmoid(logits_val) > 0.5\r\n+            # métricas por batch\r\n+            tp =   (pred & (y_val>0.5)).sum().item()\r\n+            fp =   (pred & ~(y_val>0.5)).sum().item()\r\n+            fn =   (~pred & (y_val>0.5)).sum().item()\r\n+            union = tp + fp + fn\r\n+            iou  += tp/union        if union>0   else 0\r\n+            prec += tp/(tp+fp)      if tp+fp>0   else 0\r\n+            rec  += tp/(tp+fn)      if tp+fn>0   else 0\r\n+            f1   += 2*tp/(2*tp+fp+fn) if (2*tp+fp+fn)>0 else 0\r\n+            n_val += 1\r\n+\r\n+        # medias\r\n+        val_loss = epoch_val_loss / len(val_loader)\r\n+        iou, prec, rec, f1 = [x/n_val for x in (iou,prec,rec,f1)]\r\n+        sup_avg  = epoch_sup  / len(sup_loader)\r\n+        cons_avg = epoch_cons / len(sup_loader)\r\n+        total    = sup_avg + cons_avg\r\n+        logging.info(\r\n+            f\"Época {epoch}: \"\r\n+            f\"sup={sup_avg:.4f}, cons={cons_avg:.4f}, total={total:.4f} | \"\r\n+            f\"val_loss={val_loss:.4f}, IoU={iou:.4f}, P={prec:.4f}, R={rec:.4f}, F1={f1:.4f}\"\r\n+        )\r\n+        # Guarda mejor modelo por IoU\r\n+        if iou > best_iou:\r\n+            best_iou = iou\r\n+            torch.save(student.state_dict(), f\"best_deeplab_{regime}.pth\")\r\n+    # ——— Evaluación final en TEST ———\r\n+    # Rutas de test\r\n+    xt = os.path.join(root, \"test/images\")\r\n+    yt = os.path.join(root, \"test/masks\")\r\n+    test_ds    = CorrosionDataset(xt, yt, transform=val_transform, mode=\"labeled\")\r\n+    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\r\n+\r\n+    # Carga el mejor modelo\r\n+    student.load_state_dict(torch.load(f\"best_deeplab_{regime}.pth\"))\r\n+    student.eval()\r\n+\r\n+    # Cálculo de test_loss y test_iou\r\n+    test_loss, test_iou = 0.0, 0.0\r\n+    n_test = 0\r\n+    for x_t, y_t in test_loader:\r\n+        x_t, y_t = x_t.to(DEVICE), y_t.to(DEVICE)\r\n+        with torch.no_grad():\r\n+            logits_t = student(x_t)\r\n+            loss_t = sup_loss(logits_t, y_t)\r\n+            test_loss += loss_t.item()\r\n+            pred_t = torch.sigmoid(logits_t) > 0.5\r\n+        tp = (pred_t & (y_t>0.5)).sum().item()\r\n+        union = (pred_t | (y_t>0.5)).sum().item()\r\n+        test_iou += tp/union if union>0 else 0\r\n+        n_test += 1\r\n+    test_loss /= n_test\r\n+    test_iou  /= n_test\r\n+    logging.info(f\"Test final: Loss={test_loss:.4f}, IoU={test_iou:.4f}\")\r\n+if __name__ == \"__main__\":\r\n+    # reproducibilidad\r\n+    random.seed(SEED)\r\n+    np.random.seed(SEED)\r\n+    torch.manual_seed(SEED)\r\n+    if DEVICE==\"cuda\": torch.cuda.manual_seed(SEED)\r\n+    main()\r\n"
                },
                {
                    "date": 1750678039972,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -303,310 +303,4 @@\n     np.random.seed(SEED)\r\n     torch.manual_seed(SEED)\r\n     if DEVICE==\"cuda\": torch.cuda.manual_seed(SEED)\r\n     main()\r\n-# main_semisup_pytorch.py\r\n-\r\n-import os\r\n-os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\r\n-\r\n-import sys\r\n-import argparse\r\n-import random\r\n-import logging\r\n-import numpy as np\r\n-from tqdm import tqdm\r\n-\r\n-import torch\r\n-import torch.nn as nn\r\n-import torch.optim as optim\r\n-from torch.utils.data import DataLoader, Dataset\r\n-from torchvision import transforms\r\n-from PIL import Image\r\n-\r\n-import segmentation_models_pytorch as smp\r\n-import albumentations as A\r\n-from albumentations.pytorch import ToTensorV2\r\n-from utils import crear_kernel_elipse, actualizar_teacher_ema\r\n-from losses import LossConsistenciaMorfologicaCompuesta\r\n-# ─────────── 0) Parámetros globales ───────────\r\n-SEED          = 42\r\n-BATCH_SIZE    = 2\r\n-LR            = 1e-4\r\n-EPOCHS        = 80\r\n-INPUT_SHAPE   = (384, 384)\r\n-DATA_ROOT     = r\"./data/all_results/regimes\"  # ajusta si hace falta\r\n-CLASSES       = [\"corrosion\"]\r\n-N_CLASSES     = 1\r\n-DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n-\r\n-# Mean Teacher\r\n-EMA_ALPHA     = 0.99\r\n-CONS_MAX      = 1.0\r\n-CONS_RAMPUP   = 30\r\n-UNLABELED_W   = 0.1\r\n-\r\n-def get_consistency_weight(epoch):\r\n-    if epoch >= CONS_RAMPUP:\r\n-        return CONS_MAX\r\n-    phase = 1.0 - epoch/CONS_RAMPUP\r\n-    return CONS_MAX * np.exp(-5 * phase * phase)  # :contentReference[oaicite:4]{index=4}\r\n-\r\n-# ─────────── 1) Argumentos ───────────\r\n-def parse_args():\r\n-    p = argparse.ArgumentParser()\r\n-    p.add_argument(\"--regime\", required=True,\r\n-                   choices=[\"C10\",\"C25\",\"C50\",\"C75\"],\r\n-                   help=\"Carpeta de régimen dentro de data/all_results/regimes\")\r\n-    return p.parse_args()\r\n-\r\n-# ─────────── 2) Dataset ───────────\r\n-class CorrosionDataset(Dataset):\r\n-    def __init__(self, images_dir, masks_dir, \r\n-                 transform=None, mode=\"both\"):\r\n-        self.images = sorted(os.listdir(images_dir))\r\n-        self.images_dir = images_dir\r\n-        self.masks_dir  = masks_dir\r\n-        self.transform  = transform\r\n-        self.mode       = mode  # \"labeled\", \"unlabeled\", \"both\"\r\n-\r\n-    def __len__(self):\r\n-        return len(self.images)\r\n-\r\n-    def __getitem__(self, idx):\r\n-        fname = self.images[idx]\r\n-        img = np.array(Image.open(os.path.join(self.images_dir, fname)).convert(\"RGB\"))\r\n-        if self.mode in (\"both\",\"labeled\"):\r\n-            mask = np.array(Image.open(os.path.join(self.masks_dir, fname)).convert(\"L\"))\r\n-            mask = (mask>0).astype(\"float32\")[...,None]\r\n-        else:\r\n-            mask = np.zeros((img.shape[0],img.shape[1],1),dtype=\"float32\")\r\n-\r\n-        if self.transform:\r\n-            augmented = self.transform(image=img, mask=mask)\r\n-            img = augmented[\"image\"]\r\n-            mask = augmented[\"mask\"]\r\n-            # --- aquí convertimos mask a [1, H, W] tensor ---\r\n-            if isinstance(mask, np.ndarray):\r\n-                mask = torch.from_numpy(mask).permute(2,0,1).float()\r\n-            else:\r\n-                # albumentations a veces devuelve Tensor con shape [H,W,1]\r\n-                if mask.ndim==3 and mask.shape[2]==1:\r\n-                    mask = mask.permute(2,0,1)\r\n-                elif mask.ndim==2:\r\n-                    mask = mask.unsqueeze(0)\r\n-        else:\r\n-            img = ToTensorV2()(image=img)[\"image\"]\r\n-            mask = torch.from_numpy(mask).permute(2,0,1).float()\r\n-\r\n-        return img, mask\r\n-\r\n-# ─────────── 3) Transforms ───────────\r\n-# ─────────── 3) Transforms ───────────\r\n-train_strong = A.Compose([\r\n-    A.Resize(*INPUT_SHAPE),\r\n-    A.HorizontalFlip(p=0.5),\r\n-    A.VerticalFlip(p=0.5),\r\n-    A.RandomRotate90(p=0.5),\r\n-    A.Transpose(p=0.5),\r\n-    A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\r\n-    A.RandomBrightnessContrast(p=0.5),\r\n-    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),\r\n-    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\r\n-    ToTensorV2(),\r\n-])\r\n-\r\n-train_weak = A.Compose([\r\n-    A.Resize(*INPUT_SHAPE),\r\n-    A.HorizontalFlip(p=0.2),\r\n-    A.RandomBrightnessContrast(p=0.1),\r\n-    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\r\n-    ToTensorV2(),\r\n-])\r\n-\r\n-val_transform = A.Compose([\r\n-    A.Resize(*INPUT_SHAPE),\r\n-    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\r\n-    ToTensorV2(),\r\n-])\r\n-\r\n-\r\n-# ─────────── 4) Main ───────────\r\n-def main():\r\n-    args = parse_args()\r\n-    regime = args.regime\r\n-    # Logging a archivo\r\n-    logging.basicConfig(level=logging.INFO,\r\n-                        handlers=[logging.FileHandler(\"entrenamiento_log_pytorch_enhancer.txt\"),\r\n-                                  logging.StreamHandler(sys.stdout)],\r\n-                        format=\"%(asctime)s %(levelname)s: %(message)s\")\r\n-    logging.info(f\"Dispositivo: {DEVICE}\")\r\n-\r\n-    # Rutas\r\n-    root = os.path.join(DATA_ROOT, regime)\r\n-    xl = os.path.join(root, \"train/images_labeled\")\r\n-    yl = os.path.join(root, \"train/masks_labeled\")\r\n-    xu = os.path.join(root, \"train/images_unlabeled\")\r\n-    xv = os.path.join(root, \"val/images\")\r\n-    yv = os.path.join(root, \"val/masks\")\r\n-\r\n-    # DataLoaders\r\n-    # DataLoaders\r\n-    sup_ds = CorrosionDataset(xl, yl, transform=train_strong, mode=\"labeled\")\r\n-    sup_loader = DataLoader(sup_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\r\n-\r\n-    unlab_strong_ds = CorrosionDataset(xu, xu, transform=train_strong, mode=\"unlabeled\")\r\n-    unlab_strong_loader = DataLoader(unlab_strong_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\r\n-\r\n-    unlab_weak_ds = CorrosionDataset(xu, xu, transform=train_weak, mode=\"unlabeled\")\r\n-    unlab_weak_loader = DataLoader(unlab_weak_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\r\n-\r\n-    val_ds = CorrosionDataset(xv, yv, transform=val_transform, mode=\"labeled\")\r\n-    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\r\n-\r\n-    # Modelos\r\n-    student = smp.DeepLabV3Plus(\r\n-        encoder_name=\"efficientnet-b3\",\r\n-        encoder_weights=\"imagenet\",\r\n-        in_channels=3,\r\n-        classes=N_CLASSES,\r\n-        activation=None,\r\n-    ).to(DEVICE)\r\n-\r\n-    teacher = smp.DeepLabV3Plus(\r\n-        encoder_name=\"efficientnet-b3\",\r\n-        encoder_weights=\"imagenet\",\r\n-        in_channels=3,\r\n-        classes=N_CLASSES,\r\n-        activation=None,\r\n-    ).to(DEVICE)\r\n-    teacher.load_state_dict(student.state_dict())\r\n-    for p in teacher.parameters(): p.requires_grad = False\r\n-\r\n-    # Optimizador y pérdidas\r\n-    optimizer = optim.Adam(student.parameters(), lr=LR)\r\n-    sup_loss   = smp.losses.DiceLoss(mode='binary')\r\n-    # --- TU APORTE: DISEÑO DE LA LIBRERÍA DE KERNELS BASADO EN TU ANÁLISIS ---\r\n-    # Usaremos solo un kernel de tamaño mediano y forma elíptica\r\n-    diametro_mediano = 32\r\n-    aspect_ratio_mediano = 2.34\r\n-\r\n-    # Creamos un único kernel, por ejemplo, horizontal\r\n-    kernel_unico = crear_kernel_elipse(diametro_mediano, aspect_ratio_mediano, angulo_grados=0, device=DEVICE)\r\n-\r\n-    libreria_de_kernels = [kernel_unico] # La librería ahora solo tiene un elemento\r\n-\r\n-    cons_loss = LossConsistenciaMorfologicaCompuesta(lista_kernels=libreria_de_kernels)\r\n-    logging.info(f\"Usando Loss de Consistencia Morfológica Compuesta con {len(libreria_de_kernels)} kernels.\")\r\n-\r\n-    best_iou = 0.0\r\n-    for epoch in range(1, EPOCHS+1):\r\n-        student.train()\r\n-        epoch_sup, epoch_cons = 0.0, 0.0\r\n-        loop = tqdm(zip(sup_loader, unlab_strong_loader, unlab_weak_loader), total=len(sup_loader), leave=False)\r\n-\r\n-        for (x_s, y_s), (x_us, _), (x_uw, _) in loop:\r\n-            x_s, y_s = x_s.to(DEVICE), y_s.to(DEVICE)\r\n-            x_us = x_us.to(DEVICE)   # student on strong\r\n-            x_uw = x_uw.to(DEVICE)   # teacher on weak\r\n-\r\n-            # Forward supervised\r\n-            log_s = student(x_s)\r\n-            loss_s = sup_loss(log_s, y_s)\r\n-\r\n-            # Forward unsupervised\r\n-            with torch.no_grad():\r\n-                t_uw = torch.sigmoid(teacher(x_uw))\r\n-            s_us = torch.sigmoid(student(x_us))\r\n-\r\n-            w = get_consistency_weight(epoch)\r\n-            loss_c = cons_loss(s_us, t_uw) * w * UNLABELED_W\r\n-\r\n-            loss = loss_s + loss_c\r\n-            optimizer.zero_grad()\r\n-            loss.backward()\r\n-            optimizer.step()\r\n-\r\n-            # EMA update...\r\n-\r\n-            # EMA update\r\n-            for ps, pt in zip(student.parameters(), teacher.parameters()):\r\n-                pt.data.mul_(EMA_ALPHA).add_(ps.data * (1-EMA_ALPHA))\r\n-\r\n-            epoch_sup  += loss_s.item()\r\n-            epoch_cons += loss_c.item()\r\n-            loop.set_description(f\"Epoch [{epoch}/{EPOCHS}]\")\r\n-            loop.set_postfix(loss_sup=loss_s.item(), loss_cons=loss_c.item())\r\n-        # ——— Validación (pérdida + métricas) ———\r\n-        student.eval()\r\n-        epoch_val_loss = 0.0\r\n-        iou, prec, rec, f1 = 0.0, 0.0, 0.0, 0.0\r\n-        n_val = 0\r\n-        for x_val, y_val in val_loader:\r\n-            x_val, y_val = x_val.to(DEVICE), y_val.to(DEVICE)\r\n-            with torch.no_grad():\r\n-                logits_val = student(x_val)\r\n-                # val_loss sobre BCEWithLogits\r\n-                loss_val = sup_loss(logits_val, y_val)\r\n-                epoch_val_loss += loss_val.item()\r\n-                pred = torch.sigmoid(logits_val) > 0.5\r\n-            # métricas por batch\r\n-            tp =   (pred & (y_val>0.5)).sum().item()\r\n-            fp =   (pred & ~(y_val>0.5)).sum().item()\r\n-            fn =   (~pred & (y_val>0.5)).sum().item()\r\n-            union = tp + fp + fn\r\n-            iou  += tp/union        if union>0   else 0\r\n-            prec += tp/(tp+fp)      if tp+fp>0   else 0\r\n-            rec  += tp/(tp+fn)      if tp+fn>0   else 0\r\n-            f1   += 2*tp/(2*tp+fp+fn) if (2*tp+fp+fn)>0 else 0\r\n-            n_val += 1\r\n-\r\n-        # medias\r\n-        val_loss = epoch_val_loss / len(val_loader)\r\n-        iou, prec, rec, f1 = [x/n_val for x in (iou,prec,rec,f1)]\r\n-        sup_avg  = epoch_sup  / len(sup_loader)\r\n-        cons_avg = epoch_cons / len(sup_loader)\r\n-        total    = sup_avg + cons_avg\r\n-        logging.info(\r\n-            f\"Época {epoch}: \"\r\n-            f\"sup={sup_avg:.4f}, cons={cons_avg:.4f}, total={total:.4f} | \"\r\n-            f\"val_loss={val_loss:.4f}, IoU={iou:.4f}, P={prec:.4f}, R={rec:.4f}, F1={f1:.4f}\"\r\n-        )\r\n-        # Guarda mejor modelo por IoU\r\n-        if iou > best_iou:\r\n-            best_iou = iou\r\n-            torch.save(student.state_dict(), f\"best_deeplab_{regime}.pth\")\r\n-    # ——— Evaluación final en TEST ———\r\n-    # Rutas de test\r\n-    xt = os.path.join(root, \"test/images\")\r\n-    yt = os.path.join(root, \"test/masks\")\r\n-    test_ds    = CorrosionDataset(xt, yt, transform=val_transform, mode=\"labeled\")\r\n-    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\r\n-\r\n-    # Carga el mejor modelo\r\n-    student.load_state_dict(torch.load(f\"best_deeplab_{regime}.pth\"))\r\n-    student.eval()\r\n-\r\n-    # Cálculo de test_loss y test_iou\r\n-    test_loss, test_iou = 0.0, 0.0\r\n-    n_test = 0\r\n-    for x_t, y_t in test_loader:\r\n-        x_t, y_t = x_t.to(DEVICE), y_t.to(DEVICE)\r\n-        with torch.no_grad():\r\n-            logits_t = student(x_t)\r\n-            loss_t = sup_loss(logits_t, y_t)\r\n-            test_loss += loss_t.item()\r\n-            pred_t = torch.sigmoid(logits_t) > 0.5\r\n-        tp = (pred_t & (y_t>0.5)).sum().item()\r\n-        union = (pred_t | (y_t>0.5)).sum().item()\r\n-        test_iou += tp/union if union>0 else 0\r\n-        n_test += 1\r\n-    test_loss /= n_test\r\n-    test_iou  /= n_test\r\n-    logging.info(f\"Test final: Loss={test_loss:.4f}, IoU={test_iou:.4f}\")\r\n-if __name__ == \"__main__\":\r\n-    # reproducibilidad\r\n-    random.seed(SEED)\r\n-    np.random.seed(SEED)\r\n-    torch.manual_seed(SEED)\r\n-    if DEVICE==\"cuda\": torch.cuda.manual_seed(SEED)\r\n-    main()\r\n"
                }
            ],
            "date": 1750570942975,
            "name": "Commit-0",
            "content": "# main_semisup_pytorch.py\r\n\r\nimport os\r\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\r\n\r\nimport sys\r\nimport argparse\r\nimport random\r\nimport logging\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.utils.data import DataLoader, Dataset\r\nfrom torchvision import transforms\r\nfrom PIL import Image\r\n\r\nimport segmentation_models_pytorch as smp\r\nimport albumentations as A\r\nfrom albumentations.pytorch import ToTensorV2\r\n\r\n# ─────────── 0) Parámetros globales ───────────\r\nSEED          = 42\r\nBATCH_SIZE    = 2\r\nLR            = 1e-4\r\nEPOCHS        = 80\r\nINPUT_SHAPE   = (384, 384)\r\nDATA_ROOT     = r\"./data/all_results/regimes\"  # ajusta si hace falta\r\nCLASSES       = [\"corrosion\"]\r\nN_CLASSES     = 1\r\nDEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n\r\n# Mean Teacher\r\nEMA_ALPHA     = 0.99\r\nCONS_MAX      = 1.0\r\nCONS_RAMPUP   = 30\r\nUNLABELED_W   = 1.0\r\n\r\ndef get_consistency_weight(epoch):\r\n    if epoch >= CONS_RAMPUP:\r\n        return CONS_MAX\r\n    phase = 1.0 - epoch/CONS_RAMPUP\r\n    return CONS_MAX * np.exp(-5 * phase * phase)  # :contentReference[oaicite:4]{index=4}\r\n\r\n# ─────────── 1) Argumentos ───────────\r\ndef parse_args():\r\n    p = argparse.ArgumentParser()\r\n    p.add_argument(\"--regime\", required=True,\r\n                   choices=[\"C10\",\"C25\",\"C50\",\"C75\"],\r\n                   help=\"Carpeta de régimen dentro de data/all_results/regimes\")\r\n    return p.parse_args()\r\n\r\n# ─────────── 2) Dataset ───────────\r\nclass CorrosionDataset(Dataset):\r\n    def __init__(self, images_dir, masks_dir, \r\n                 transform=None, mode=\"both\"):\r\n        self.images = sorted(os.listdir(images_dir))\r\n        self.images_dir = images_dir\r\n        self.masks_dir  = masks_dir\r\n        self.transform  = transform\r\n        self.mode       = mode  # \"labeled\", \"unlabeled\", \"both\"\r\n\r\n    def __len__(self):\r\n        return len(self.images)\r\n\r\n    def __getitem__(self, idx):\r\n        fname = self.images[idx]\r\n        img = np.array(Image.open(os.path.join(self.images_dir, fname)).convert(\"RGB\"))\r\n        if self.mode in (\"both\",\"labeled\"):\r\n            mask = np.array(Image.open(os.path.join(self.masks_dir, fname)).convert(\"L\"))\r\n            mask = (mask>0).astype(\"float32\")[...,None]\r\n        else:\r\n            mask = np.zeros((img.shape[0],img.shape[1],1),dtype=\"float32\")\r\n\r\n        if self.transform:\r\n            augmented = self.transform(image=img, mask=mask)\r\n            img = augmented[\"image\"]\r\n            mask = augmented[\"mask\"]\r\n            # --- aquí convertimos mask a [1, H, W] tensor ---\r\n            if isinstance(mask, np.ndarray):\r\n                mask = torch.from_numpy(mask).permute(2,0,1).float()\r\n            else:\r\n                # albumentations a veces devuelve Tensor con shape [H,W,1]\r\n                if mask.ndim==3 and mask.shape[2]==1:\r\n                    mask = mask.permute(2,0,1)\r\n                elif mask.ndim==2:\r\n                    mask = mask.unsqueeze(0)\r\n        else:\r\n            img = ToTensorV2()(image=img)[\"image\"]\r\n            mask = torch.from_numpy(mask).permute(2,0,1).float()\r\n\r\n        return img, mask\r\n\r\n# ─────────── 3) Transforms ───────────\r\n# ─────────── 3) Transforms ───────────\r\ntrain_strong = A.Compose([\r\n    A.Resize(*INPUT_SHAPE),\r\n    A.HorizontalFlip(p=0.5),\r\n    A.VerticalFlip(p=0.5),\r\n    A.RandomRotate90(p=0.5),\r\n    A.Transpose(p=0.5),\r\n    A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\r\n    A.RandomBrightnessContrast(p=0.5),\r\n    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),\r\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\r\n    ToTensorV2(),\r\n])\r\n\r\ntrain_weak = A.Compose([\r\n    A.Resize(*INPUT_SHAPE),\r\n    A.HorizontalFlip(p=0.2),\r\n    A.RandomBrightnessContrast(p=0.1),\r\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\r\n    ToTensorV2(),\r\n])\r\n\r\nval_transform = A.Compose([\r\n    A.Resize(*INPUT_SHAPE),\r\n    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\r\n    ToTensorV2(),\r\n])\r\n\r\n\r\n# ─────────── 4) Main ───────────\r\ndef main():\r\n    args = parse_args()\r\n    regime = args.regime\r\n    # Logging a archivo\r\n    logging.basicConfig(level=logging.INFO,\r\n                        handlers=[logging.FileHandler(\"entrenamiento_log_pytorch.txt\"),\r\n                                  logging.StreamHandler(sys.stdout)],\r\n                        format=\"%(asctime)s %(levelname)s: %(message)s\")\r\n    logging.info(f\"Dispositivo: {DEVICE}\")\r\n\r\n    # Rutas\r\n    root = os.path.join(DATA_ROOT, regime)\r\n    xl = os.path.join(root, \"train/images_labeled\")\r\n    yl = os.path.join(root, \"train/masks_labeled\")\r\n    xu = os.path.join(root, \"train/images_unlabeled\")\r\n    xv = os.path.join(root, \"val/images\")\r\n    yv = os.path.join(root, \"val/masks\")\r\n\r\n    # DataLoaders\r\n    # DataLoaders\r\n    sup_ds = CorrosionDataset(xl, yl, transform=train_strong, mode=\"labeled\")\r\n    sup_loader = DataLoader(sup_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\r\n\r\n    unlab_strong_ds = CorrosionDataset(xu, xu, transform=train_strong, mode=\"unlabeled\")\r\n    unlab_strong_loader = DataLoader(unlab_strong_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\r\n\r\n    unlab_weak_ds = CorrosionDataset(xu, xu, transform=train_weak, mode=\"unlabeled\")\r\n    unlab_weak_loader = DataLoader(unlab_weak_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\r\n\r\n    val_ds = CorrosionDataset(xv, yv, transform=val_transform, mode=\"labeled\")\r\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\r\n\r\n    # Modelos\r\n    student = smp.DeepLabV3Plus(\r\n        encoder_name=\"efficientnet-b3\",\r\n        encoder_weights=\"imagenet\",\r\n        in_channels=3,\r\n        classes=N_CLASSES,\r\n        activation=None,\r\n    ).to(DEVICE)\r\n\r\n    teacher = smp.DeepLabV3Plus(\r\n        encoder_name=\"efficientnet-b3\",\r\n        encoder_weights=\"imagenet\",\r\n        in_channels=3,\r\n        classes=N_CLASSES,\r\n        activation=None,\r\n    ).to(DEVICE)\r\n    teacher.load_state_dict(student.state_dict())\r\n    for p in teacher.parameters(): p.requires_grad = False\r\n\r\n    # Optimizador y pérdidas\r\n    optimizer = optim.Adam(student.parameters(), lr=LR)\r\n    sup_loss   = smp.losses.DiceLoss(mode='binary')\r\n    cons_loss  = nn.MSELoss()\r\n\r\n    best_iou = 0.0\r\n    for epoch in range(1, EPOCHS+1):\r\n        student.train()\r\n        epoch_sup, epoch_cons = 0.0, 0.0\r\n \r\n        for (x_s, y_s), (x_us, _), (x_uw, _) in zip(sup_loader, unlab_strong_loader, unlab_weak_loader):\r\n            x_s, y_s = x_s.to(DEVICE), y_s.to(DEVICE)\r\n            x_us = x_us.to(DEVICE)   # student on strong\r\n            x_uw = x_uw.to(DEVICE)   # teacher on weak\r\n\r\n            # Forward supervised\r\n            log_s = student(x_s)\r\n            loss_s = sup_loss(log_s, y_s)\r\n\r\n            # Forward unsupervised\r\n            with torch.no_grad():\r\n                t_uw = torch.sigmoid(teacher(x_uw))\r\n            s_us = torch.sigmoid(student(x_us))\r\n\r\n            w = get_consistency_weight(epoch)\r\n            loss_c = cons_loss(s_us, t_uw) * w * UNLABELED_W\r\n\r\n            loss = loss_s + loss_c\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n            optimizer.step()\r\n\r\n            # EMA update...\r\n\r\n            # EMA update\r\n            for ps, pt in zip(student.parameters(), teacher.parameters()):\r\n                pt.data.mul_(EMA_ALPHA).add_(ps.data * (1-EMA_ALPHA))\r\n\r\n            epoch_sup  += loss_s.item()\r\n            epoch_cons += loss_c.item()\r\n\r\n        # ——— Validación (pérdida + métricas) ———\r\n        student.eval()\r\n        epoch_val_loss = 0.0\r\n        iou, prec, rec, f1 = 0.0, 0.0, 0.0, 0.0\r\n        n_val = 0\r\n        for x_val, y_val in val_loader:\r\n            x_val, y_val = x_val.to(DEVICE), y_val.to(DEVICE)\r\n            with torch.no_grad():\r\n                logits_val = student(x_val)\r\n                # val_loss sobre BCEWithLogits\r\n                loss_val = sup_loss(logits_val, y_val)\r\n                epoch_val_loss += loss_val.item()\r\n                pred = torch.sigmoid(logits_val) > 0.5\r\n            # métricas por batch\r\n            tp =   (pred & (y_val>0.5)).sum().item()\r\n            fp =   (pred & ~(y_val>0.5)).sum().item()\r\n            fn =   (~pred & (y_val>0.5)).sum().item()\r\n            union = tp + fp + fn\r\n            iou  += tp/union        if union>0   else 0\r\n            prec += tp/(tp+fp)      if tp+fp>0   else 0\r\n            rec  += tp/(tp+fn)      if tp+fn>0   else 0\r\n            f1   += 2*tp/(2*tp+fp+fn) if (2*tp+fp+fn)>0 else 0\r\n            n_val += 1\r\n\r\n        # medias\r\n        val_loss = epoch_val_loss / len(val_loader)\r\n        iou, prec, rec, f1 = [x/n_val for x in (iou,prec,rec,f1)]\r\n        sup_avg  = epoch_sup  / len(sup_loader)\r\n        cons_avg = epoch_cons / len(sup_loader)\r\n        total    = sup_avg + cons_avg\r\n        logging.info(\r\n            f\"Época {epoch}: \"\r\n            f\"sup={sup_avg:.4f}, cons={cons_avg:.4f}, total={total:.4f} | \"\r\n            f\"val_loss={val_loss:.4f}, IoU={iou:.4f}, P={prec:.4f}, R={rec:.4f}, F1={f1:.4f}\"\r\n        )\r\n        # Guarda mejor modelo por IoU\r\n        if iou > best_iou:\r\n            best_iou = iou\r\n            torch.save(student.state_dict(), f\"best_deeplab_{regime}.pth\")\r\n    # ——— Evaluación final en TEST ———\r\n    # Rutas de test\r\n    xt = os.path.join(root, \"test/images\")\r\n    yt = os.path.join(root, \"test/masks\")\r\n    test_ds    = CorrosionDataset(xt, yt, transform=val_transform, mode=\"labeled\")\r\n    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\r\n\r\n    # Carga el mejor modelo\r\n    student.load_state_dict(torch.load(f\"best_deeplab_{regime}.pth\"))\r\n    student.eval()\r\n\r\n    # Cálculo de test_loss y test_iou\r\n    test_loss, test_iou = 0.0, 0.0\r\n    n_test = 0\r\n    for x_t, y_t in test_loader:\r\n        x_t, y_t = x_t.to(DEVICE), y_t.to(DEVICE)\r\n        with torch.no_grad():\r\n            logits_t = student(x_t)\r\n            loss_t = sup_loss(logits_t, y_t)\r\n            test_loss += loss_t.item()\r\n            pred_t = torch.sigmoid(logits_t) > 0.5\r\n        tp = (pred_t & (y_t>0.5)).sum().item()\r\n        union = (pred_t | (y_t>0.5)).sum().item()\r\n        test_iou += tp/union if union>0 else 0\r\n        n_test += 1\r\n    test_loss /= n_test\r\n    test_iou  /= n_test\r\n    logging.info(f\"Test final: Loss={test_loss:.4f}, IoU={test_iou:.4f}\")\r\nif __name__ == \"__main__\":\r\n    # reproducibilidad\r\n    random.seed(SEED)\r\n    np.random.seed(SEED)\r\n    torch.manual_seed(SEED)\r\n    if DEVICE==\"cuda\": torch.cuda.manual_seed(SEED)\r\n    main()\r\n"
        }
    ]
}