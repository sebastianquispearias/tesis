{
    "sourceFile": "main_supervised_pythorch.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 10,
            "patches": [
                {
                    "date": 1750633715595,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1750633895727,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -53,14 +53,22 @@\n         \r\n         return img, mask.unsqueeze(0) # Añadir dimensión de canal a la máscara\r\n \r\n # Augmentations (consistente con tu otro script)\r\n+\r\n train_transform = A.Compose([\r\n     A.Resize(*INPUT_SHAPE),\r\n+    # Geométricas\r\n     A.HorizontalFlip(p=0.5),\r\n     A.VerticalFlip(p=0.5),\r\n     A.RandomRotate90(p=0.5),\r\n-    A.GridDistortion(p=0.2),\r\n+    A.Transpose(p=0.5), # Añadido para consistencia\r\n+    A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\r\n+    # Fotométricas\r\n+    A.RandomBrightnessContrast(p=0.5),\r\n+    # Enmascarado\r\n+    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),\r\n+    # Normalización y conversión a Tensor\r\n     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\r\n     ToTensorV2(),\r\n ])\r\n val_transform = A.Compose([\r\n"
                },
                {
                    "date": 1750633994052,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,5 +1,6 @@\n import os\r\n+s.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\r\n import sys\r\n import argparse\r\n import random\r\n import numpy as np\r\n"
                },
                {
                    "date": 1750634007196,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,6 +1,6 @@\n import os\r\n-s.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\r\n+os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\r\n import sys\r\n import argparse\r\n import random\r\n import numpy as np\r\n"
                },
                {
                    "date": 1750634044259,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,9 +11,9 @@\n import albumentations as A\r\n from albumentations.pytorch import ToTensorV2\r\n from tqdm import tqdm\r\n import segmentation_models_pytorch as smp\r\n-\r\n+import logging\r\n # ----------------------------------------------------\r\n # 0) CONFIGURACIÓN GLOBAL Y SEED\r\n # ----------------------------------------------------\r\n SEED = 42\r\n"
                },
                {
                    "date": 1750634176670,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -117,21 +117,31 @@\n     # Lógica de congelamiento del encoder\r\n     freeze_encoder = regime.startswith(\"B\")\r\n     logging.info(f\"Congelar encoder (freeze_encoder): {freeze_encoder}\")\r\n \r\n-    # Modelo\r\n+    # --- CAMBIO IMPORTANTE: Lógica de creación y congelamiento del modelo ---\r\n+\r\n+    # 1. Creamos el modelo SIN el argumento 'encoder_freeze'\r\n     model = smp.DeepLabV3Plus(\r\n         encoder_name=BACKBONE,\r\n         encoder_weights=\"imagenet\",\r\n         in_channels=3,\r\n-        classes=N_CLASSES,\r\n-        encoder_freeze=freeze_encoder # <-- Lógica de congelamiento aplicada aquí\r\n+        classes=N_CLASSES\r\n     ).to(DEVICE)\r\n+\r\n+    # 2. Si el régimen es \"Bxx\", congelamos el encoder manualmente\r\n+    if freeze_encoder:\r\n+        for param in model.encoder.parameters():\r\n+            param.requires_grad = False\r\n+        logging.info(\"--> Encoder congelado manualmente. Solo se entrenará el decoder.\")\r\n     \r\n-    # Loss y Optimizador\r\n+    # 3. Creamos el optimizador DESPUÉS de congelar, pasándole solo los parámetros entrenables\r\n+    # Esto es más eficiente y la forma correcta de hacerlo.\r\n+    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\r\n+    optimizer = optim.Adam(trainable_params, lr=LR)\r\n+\r\n+    # Loss (sin cambios)\r\n     loss_fn = smp.losses.DiceLoss(mode='binary')\r\n-    optimizer = optim.Adam(model.parameters(), lr=LR)\r\n-    \r\n     # Métricas\r\n     metrics = {\r\n         \"iou\": smp.metrics.iou_score,\r\n         \"f1\": smp.metrics.f1_score\r\n"
                },
                {
                    "date": 1750634314939,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -110,9 +110,9 @@\n     \r\n     # DataLoaders\r\n     train_ds = CorrosionDataset(os.path.join(train_dir, \"images_labeled\"), os.path.join(train_dir, \"masks_labeled\"), transform=train_transform)\r\n     val_ds = CorrosionDataset(os.path.join(val_dir, \"images\"), os.path.join(val_dir, \"masks\"), transform=val_transform)\r\n-    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\r\n+    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\r\n     val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\r\n \r\n     # Lógica de congelamiento del encoder\r\n     freeze_encoder = regime.startswith(\"B\")\r\n"
                },
                {
                    "date": 1750634441290,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -175,17 +175,30 @@\n         # Validación\r\n         model.eval()\r\n         val_iou, val_f1, val_loss = 0.0, 0.0, 0.0\r\n         with torch.no_grad():\r\n-            for x_val, y_val in val_loader:\r\n-                x_val, y_val = x_val.to(DEVICE), y_val.to(DEVICE)\r\n-                pred_val = model(x_val)\r\n-                val_loss += loss_fn(pred_val, y_val).item()\r\n-                # Para métricas, las predicciones deben ser probabilidades y las máscaras enteros\r\n-                probs = torch.sigmoid(pred_val)\r\n-                val_iou += metrics[\"iou\"](probs, y_val.long()).item()\r\n-                val_f1 += metrics[\"f1\"](probs, y_val.long()).item()\r\n+                    for x_val, y_val in val_loader:\r\n+                        x_val, y_val = x_val.to(DEVICE), y_val.to(DEVICE)\r\n+                        \r\n+                        # 1. Obtener predicción\r\n+                        pred_val = model(x_val)\r\n+                        val_loss_batch = loss_fn(pred_val, y_val)\r\n+                        \r\n+                        # --- CORRECCIÓN: CÁLCULO DE MÉTRICAS EN DOS PASOS ---\r\n+                        probs = torch.sigmoid(pred_val)\r\n+                        y_val_int = y_val.long()\r\n \r\n+                        # Paso A: Calcular los componentes base (tp, fp, fn, tn)\r\n+                        tp, fp, fn, tn = smp.metrics.get_stats(probs, y_val_int, mode='binary', threshold=0.5)\r\n+\r\n+                        # Paso B: Usar los componentes para calcular las métricas de score\r\n+                        val_iou_batch = smp.metrics.iou_score(tp, fp, fn, tn, reduction='macro-mean')\r\n+                        val_f1_batch = smp.metrics.f1_score(tp, fp, fn, tn, reduction='macro-mean')\r\n+                        \r\n+                        # Acumular los resultados del lote\r\n+                        val_loss += val_loss_batch.item()\r\n+                        val_iou += val_iou_batch.item()\r\n+                        val_f1 += val_f1_batch.item()\r\n         # Promediar métricas de validación\r\n         val_iou /= len(val_loader)\r\n         val_f1 /= len(val_loader)\r\n         val_loss /= len(val_loader)\r\n"
                },
                {
                    "date": 1750634528711,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -189,12 +189,12 @@\n \r\n                         # Paso A: Calcular los componentes base (tp, fp, fn, tn)\r\n                         tp, fp, fn, tn = smp.metrics.get_stats(probs, y_val_int, mode='binary', threshold=0.5)\r\n \r\n+                        val_f1_batch = smp.metrics.f1_score(tp, fp, fn, tn, reduction='macro-mean')\r\n                         # Paso B: Usar los componentes para calcular las métricas de score\r\n-                        val_iou_batch = smp.metrics.iou_score(tp, fp, fn, tn, reduction='macro-mean')\r\n-                        val_f1_batch = smp.metrics.f1_score(tp, fp, fn, tn, reduction='macro-mean')\r\n-                        \r\n+                        val_iou_batch = smp.metrics.iou_score(tp, fp, fn, tn, reduction='macro')\r\n+                        val_f1_batch = smp.metrics.f1_score(tp, fp, fn, tn, reduction='macro')\r\n                         # Acumular los resultados del lote\r\n                         val_loss += val_loss_batch.item()\r\n                         val_iou += val_iou_batch.item()\r\n                         val_f1 += val_f1_batch.item()\r\n"
                },
                {
                    "date": 1750634635644,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -189,9 +189,8 @@\n \r\n                         # Paso A: Calcular los componentes base (tp, fp, fn, tn)\r\n                         tp, fp, fn, tn = smp.metrics.get_stats(probs, y_val_int, mode='binary', threshold=0.5)\r\n \r\n-                        val_f1_batch = smp.metrics.f1_score(tp, fp, fn, tn, reduction='macro-mean')\r\n                         # Paso B: Usar los componentes para calcular las métricas de score\r\n                         val_iou_batch = smp.metrics.iou_score(tp, fp, fn, tn, reduction='macro')\r\n                         val_f1_batch = smp.metrics.f1_score(tp, fp, fn, tn, reduction='macro')\r\n                         # Acumular los resultados del lote\r\n"
                },
                {
                    "date": 1750636784527,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -211,6 +211,42 @@\n \r\n     logging.info(f\"Entrenamiento completado. Mejor Val IoU: {best_iou:.4f}\")\r\n     # Aquí iría la evaluación final sobre el conjunto de test, cargando el mejor modelo guardado.\r\n \r\n+    # --- AÑADIR ESTE BLOQUE PARA LA EVALUACIÓN FINAL EN TEST ---\r\n+    logging.info(\"--- Iniciando Evaluación Final en el Conjunto de Test ---\")\r\n+    \r\n+    # 1. Cargar el mejor modelo guardado\r\n+    model.load_state_dict(torch.load(model_save_path))\r\n+    logging.info(f\"Cargando el mejor modelo desde {model_save_path}\")\r\n+    \r\n+    # 2. Crear el DataLoader de Test\r\n+    test_dir = os.path.join(DATA_ROOT, regime, \"test\")\r\n+    test_ds = CorrosionDataset(os.path.join(test_dir, \"images\"), os.path.join(test_dir, \"masks\"), transform=val_transform)\r\n+    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\r\n+\r\n+    # 3. Evaluar en el conjunto de Test\r\n+    model.eval()\r\n+    test_iou, test_f1, test_loss = 0.0, 0.0, 0.0\r\n+    with torch.no_grad():\r\n+        for x_test, y_test in test_loader:\r\n+            x_test, y_test = x_test.to(DEVICE), y_test.to(DEVICE)\r\n+            pred_test = model(x_test)\r\n+            test_loss += loss_fn(pred_test, y_test).item()\r\n+            probs = torch.sigmoid(pred_test)\r\n+            y_test_int = y_test.long()\r\n+            \r\n+            tp, fp, fn, tn = smp.metrics.get_stats(probs, y_test_int, mode='binary', threshold=0.5)\r\n+            test_iou += smp.metrics.iou_score(tp, fp, fn, tn, reduction='macro').item()\r\n+            test_f1 += smp.metrics.f1_score(tp, fp, fn, tn, reduction='macro').item()\r\n+\r\n+    # 4. Imprimir los resultados finales\r\n+    test_iou /= len(test_loader)\r\n+    test_f1 /= len(test_loader)\r\n+    test_loss /= len(test_loader)\r\n+    \r\n+    logging.info(f\">> RESULTADO FINAL EN TEST: Loss={test_loss:.4f}, IoU={test_iou:.4f}, F1={test_f1:.4f}\")\r\n+    logging.info(\"¡Proceso finalizado con éxito!\")\r\n+\r\n+# ... (el `if __name__ == '__main__':` sigue igual)\r\n if __name__ == '__main__':\r\n     main()\n\\ No newline at end of file\n"
                }
            ],
            "date": 1750633715595,
            "name": "Commit-0",
            "content": "import os\r\nimport sys\r\nimport argparse\r\nimport random\r\nimport numpy as np\r\nimport torch\r\nimport torch.optim as optim\r\nfrom torch.utils.data import Dataset, DataLoader\r\nimport cv2\r\nimport albumentations as A\r\nfrom albumentations.pytorch import ToTensorV2\r\nfrom tqdm import tqdm\r\nimport segmentation_models_pytorch as smp\r\n\r\n# ----------------------------------------------------\r\n# 0) CONFIGURACIÓN GLOBAL Y SEED\r\n# ----------------------------------------------------\r\nSEED = 42\r\nrandom.seed(SEED)\r\nnp.random.seed(SEED)\r\ntorch.manual_seed(SEED)\r\ntorch.cuda.manual_seed(SEED)\r\n\r\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\nDATA_ROOT = \"data/all_results/regimes\" # Asegúrate que esta sea tu ruta base\r\nBACKBONE = 'efficientnet-b3'\r\nINPUT_SHAPE = (384, 384)\r\nLR = 1e-4\r\nBATCH_SIZE = 2 # Ajusta según tu VRAM (con 12GB, 2 o 4 debería funcionar)\r\nEPOCHS = 80\r\nN_CLASSES = 1 # Segmentación binaria\r\n\r\n# ----------------------------------------------------\r\n# A) DATASET Y DATALOADERS\r\n# ----------------------------------------------------\r\nclass CorrosionDataset(Dataset):\r\n    def __init__(self, images_dir, masks_dir, transform=None):\r\n        self.images_fps = sorted([os.path.join(images_dir, fname) for fname in os.listdir(images_dir)])\r\n        self.masks_fps = sorted([os.path.join(masks_dir, fname) for fname in os.listdir(masks_dir)])\r\n        self.transform = transform\r\n\r\n    def __len__(self):\r\n        return len(self.images_fps)\r\n\r\n    def __getitem__(self, idx):\r\n        img = cv2.cvtColor(cv2.imread(self.images_fps[idx]), cv2.COLOR_BGR2RGB)\r\n        mask = cv2.imread(self.masks_fps[idx], cv2.IMREAD_GRAYSCALE)\r\n        mask = (mask > 0).astype('float32')\r\n\r\n        if self.transform:\r\n            augmented = self.transform(image=img, mask=mask)\r\n            img, mask = augmented['image'], augmented['mask']\r\n        \r\n        return img, mask.unsqueeze(0) # Añadir dimensión de canal a la máscara\r\n\r\n# Augmentations (consistente con tu otro script)\r\ntrain_transform = A.Compose([\r\n    A.Resize(*INPUT_SHAPE),\r\n    A.HorizontalFlip(p=0.5),\r\n    A.VerticalFlip(p=0.5),\r\n    A.RandomRotate90(p=0.5),\r\n    A.GridDistortion(p=0.2),\r\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\r\n    ToTensorV2(),\r\n])\r\nval_transform = A.Compose([\r\n    A.Resize(*INPUT_SHAPE),\r\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\r\n    ToTensorV2(),\r\n])\r\n\r\n# ----------------------------------------------------\r\n# B) PARSEO DE ARGUMENTOS\r\n# ----------------------------------------------------\r\ndef parse_args():\r\n    p = argparse.ArgumentParser(description=\"Entrenamiento supervisado para baselines Bxx\")\r\n    p.add_argument(\"--regime\", required=True,\r\n                   choices=[\"B10\", \"B25\", \"B50\", \"B75\", \"A-Full\"],\r\n                   help=\"Régimen de datos a utilizar.\")\r\n    return p.parse_args()\r\n\r\n# ----------------------------------------------------\r\n# C) BUCLE DE ENTRENAMIENTO Y VALIDACIÓN\r\n# ----------------------------------------------------\r\ndef main():\r\n    args = parse_args()\r\n    regime = args.regime\r\n    \r\n    # Configurar logging\r\n    log_file = f\"entrenamiento_log_{regime}_SUPERVISADO.txt\"\r\n    logging.basicConfig(level=logging.INFO,\r\n                        handlers=[logging.FileHandler(log_file), logging.StreamHandler(sys.stdout)],\r\n                        format=\"%(asctime)s %(levelname)s: %(message)s\")\r\n    \r\n    logging.info(f\"--- Iniciando Entrenamiento Supervisado para el Régimen: {regime} ---\")\r\n    logging.info(f\"Usando dispositivo: {DEVICE}\")\r\n\r\n    # Rutas\r\n    train_dir = os.path.join(DATA_ROOT, regime, \"train\")\r\n    val_dir = os.path.join(DATA_ROOT, regime, \"val\")\r\n    \r\n    # DataLoaders\r\n    train_ds = CorrosionDataset(os.path.join(train_dir, \"images_labeled\"), os.path.join(train_dir, \"masks_labeled\"), transform=train_transform)\r\n    val_ds = CorrosionDataset(os.path.join(val_dir, \"images\"), os.path.join(val_dir, \"masks\"), transform=val_transform)\r\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\r\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\r\n\r\n    # Lógica de congelamiento del encoder\r\n    freeze_encoder = regime.startswith(\"B\")\r\n    logging.info(f\"Congelar encoder (freeze_encoder): {freeze_encoder}\")\r\n\r\n    # Modelo\r\n    model = smp.DeepLabV3Plus(\r\n        encoder_name=BACKBONE,\r\n        encoder_weights=\"imagenet\",\r\n        in_channels=3,\r\n        classes=N_CLASSES,\r\n        encoder_freeze=freeze_encoder # <-- Lógica de congelamiento aplicada aquí\r\n    ).to(DEVICE)\r\n    \r\n    # Loss y Optimizador\r\n    loss_fn = smp.losses.DiceLoss(mode='binary')\r\n    optimizer = optim.Adam(model.parameters(), lr=LR)\r\n    \r\n    # Métricas\r\n    metrics = {\r\n        \"iou\": smp.metrics.iou_score,\r\n        \"f1\": smp.metrics.f1_score\r\n    }\r\n\r\n    best_iou = 0.0\r\n    model_save_path = f\"best_model_supervised_{regime}.pth\"\r\n\r\n    # Bucle Principal\r\n    for epoch in range(1, EPOCHS + 1):\r\n        model.train()\r\n        loop = tqdm(train_loader, total=len(train_loader), leave=False)\r\n        epoch_loss = 0.0\r\n        \r\n        for x, y in loop:\r\n            x, y = x.to(DEVICE), y.to(DEVICE)\r\n            \r\n            # Forward\r\n            pred = model(x)\r\n            loss = loss_fn(pred, y)\r\n            \r\n            # Backward\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n            optimizer.step()\r\n            \r\n            epoch_loss += loss.item()\r\n            loop.set_description(f\"Epoch [{epoch}/{EPOCHS}]\")\r\n            loop.set_postfix(loss=loss.item())\r\n\r\n        # Validación\r\n        model.eval()\r\n        val_iou, val_f1, val_loss = 0.0, 0.0, 0.0\r\n        with torch.no_grad():\r\n            for x_val, y_val in val_loader:\r\n                x_val, y_val = x_val.to(DEVICE), y_val.to(DEVICE)\r\n                pred_val = model(x_val)\r\n                val_loss += loss_fn(pred_val, y_val).item()\r\n                # Para métricas, las predicciones deben ser probabilidades y las máscaras enteros\r\n                probs = torch.sigmoid(pred_val)\r\n                val_iou += metrics[\"iou\"](probs, y_val.long()).item()\r\n                val_f1 += metrics[\"f1\"](probs, y_val.long()).item()\r\n\r\n        # Promediar métricas de validación\r\n        val_iou /= len(val_loader)\r\n        val_f1 /= len(val_loader)\r\n        val_loss /= len(val_loader)\r\n\r\n        logging.info(f\"Epoch {epoch}: Train Loss={epoch_loss/len(train_loader):.4f} | Val Loss={val_loss:.4f} | Val IoU={val_iou:.4f} | Val F1={val_f1:.4f}\")\r\n\r\n        if val_iou > best_iou:\r\n            best_iou = val_iou\r\n            torch.save(model.state_dict(), model_save_path)\r\n            logging.info(f\"--> Nuevo mejor modelo guardado en {model_save_path} con Val IoU: {best_iou:.4f}\")\r\n\r\n    logging.info(f\"Entrenamiento completado. Mejor Val IoU: {best_iou:.4f}\")\r\n    # Aquí iría la evaluación final sobre el conjunto de test, cargando el mejor modelo guardado.\r\n\r\nif __name__ == '__main__':\r\n    main()"
        }
    ]
}