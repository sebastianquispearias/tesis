{
    "sourceFile": "main_semisup_l copy.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1750022171428,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1750022220322,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -551,10 +551,12 @@\n                             loss_sup  = dice_loss(y_lab, s_lab_probs)\r\n \r\n                             # DEBUG:\r\n                             logging.info(f\"DEBUG dtype y_lab: {y_lab.dtype}, dtype s_lab_probs: {s_lab_probs.dtype}\")\r\n-                            logging.inf\r\n+                            logging.finfo(f\"DEBUG valores únicos y_lab: {tf.unique(tf.reshape(y_lab, [-1])).y.numpy()[:5]}\")\r\n \r\n+                            logging.info(f\"DEBUG sup loss (primer batch): {loss_sup.numpy()}\")\r\n+\r\n                             # Student en unlabeled\r\n                             s_unl_preds = student(x_unl_s, training=True)\r\n                             s_unl_probs = tf.sigmoid(s_unl_preds)\r\n                             loss_cons = mse_loss(t_probs, s_unl_probs)\r\n"
                }
            ],
            "date": 1750022171428,
            "name": "Commit-0",
            "content": "#!/usr/bin/env python3\r\n# -*- coding: utf-8 -*-\r\n\"\"\"\r\nmain_final_ultimoPCnuevosemisuperv.py – Entrenamiento supervisado (A-Full, Bxx)\r\ny semi-supervisado (Cxx, Mean Teacher)\r\n\r\nUso:\r\n  python main_final_ultimoPCnuevosemisuperv.py --mode supervised --regime B25\r\n  python main_final_ultimoPCnuevosemisuperv.py --mode semi       --regime C25\r\n  python main_semisup_gtestaug.py --mode semi --regime C75\r\n\r\n\"\"\"\r\n\r\nimport os\r\nimport sys\r\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\r\n\r\nimport argparse\r\nimport logging\r\nimport random\r\nimport numpy as np\r\nimport tensorflow as tf\r\n# Evita que TF reserve toda la memoria GPU de golpe\r\ngpus = tf.config.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\nfrom tensorflow.keras import mixed_precision\r\nmixed_precision.set_global_policy('float32')\r\nimport cv2\r\nimport csv\r\nfrom metrics_utils import evaluate_and_log\r\n\r\nfrom pathlib import Path\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import backend as K\r\nimport segmentation_models as sm    \r\nfrom sklearn.model_selection import train_test_split\r\n\r\nfrom callbacks_monitor import ProgressMonitor\r\nfrom models import build_model\r\n\r\nimport albumentations as A\r\n\r\ndef get_strong_augmentation(input_shape):\r\n    \"\"\"Transformaciones fuertes para el ESTUDIANTE.\"\"\"\r\n    return A.Compose([\r\n        # 1) Redimensionar\r\n        A.Resize(*INPUT_SHAPE[:2]),\r\n        # 2) Geométricas\r\n        A.HorizontalFlip(p=0.5),\r\n        A.VerticalFlip(p=0.5),\r\n        A.RandomRotate90(p=0.5),\r\n        A.Transpose(p=0.5),\r\n        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\r\n        # 3) Fotométricas\r\n        A.RandomBrightnessContrast(p=0.5),\r\n        # 4) Enmascarado\r\n        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),\r\n    ])\r\n\r\ndef get_weak_augmentation(input_shape):\r\n    \"\"\"Transformaciones débiles para el PROFESOR.\"\"\"\r\n    return A.Compose([\r\n        A.Resize(*input_shape[:2]),\r\n        A.HorizontalFlip(p=0.2),            # flip suave :contentReference[oaicite:0]{index=0}\r\n        A.RandomBrightnessContrast(p=0.1),\r\n    ])\r\n\r\n\r\n\r\n# ──────────────────────────────────────────────────────────────────────────────\r\n# 0) OPCIONES DE TF y SEEDING\r\n# ──────────────────────────────────────────────────────────────────────────────\r\nSEED = 42\r\nos.environ['PYTHONHASHSEED'] = str(SEED)\r\nrandom.seed(SEED)\r\nnp.random.seed(SEED)\r\ntf.random.set_seed(SEED)\r\n\r\ntf.config.run_functions_eagerly(True)\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n# ──────────────────────────────────────────────────────────────────────────────\r\n# A) CONFIGURACIÓN DE LOGGING\r\n# ──────────────────────────────────────────────────────────────────────────────\r\nlog = open('entrenamiento_log.txt', 'w', encoding='utf-8')\r\nsys.stdout = log\r\nsys.stderr = log\r\nlogging.basicConfig(\r\n    level=logging.INFO,\r\n    format='%(asctime)s %(levelname)s %(message)s',\r\n    handlers=[logging.StreamHandler(log)]\r\n)\r\n\r\n# ──────────────────────────────────────────────────────────────────────────────\r\n# B) PARÁMETROS GLOBALES\r\n# ──────────────────────────────────────────────────────────────────────────────\r\nBACKBONE    = 'efficientnetb3'\r\nBATCH_SIZE  = 4\r\nCLASSES     = ['corrosion']\r\nLR          = 1e-4\r\nEPOCHS      = 8\r\nINPUT_SHAPE = (384, 384, 3)\r\nn_classes   = 1 if len(CLASSES) == 1 else (len(CLASSES) + 1)\r\nactivation  = 'sigmoid' if n_classes == 1 else 'softmax'\r\n\r\nARCHITECTURES = [ 'fpn']  # usado en modo supervisado\r\n\r\n# Parámetros Mean Teacher\r\nEMA_ALPHA        = 0.99\r\nCONS_MAX         = 1.0\r\nCONS_RAMPUP      = 30\r\nUNLABELED_WEIGHT = 1.0\r\n\r\n# ──────────────────────────────────────────────────────────────────────────────\r\n# C) AUGMENTATIONS Y PREPROCESSING\r\n# ──────────────────────────────────────────────────────────────────────────────\r\npreprocess_input = sm.get_preprocessing(BACKBONE)\r\n\r\ndef get_training_augmentation():\r\n    return A.Compose([\r\n    # 1) Redimensionar\r\n        A.Resize(*input_shape[:2]),\r\n        # 2) Geométricas\r\n        A.HorizontalFlip(p=0.5),\r\n        A.VerticalFlip(p=0.5),\r\n        A.RandomRotate90(p=0.5),\r\n        A.Transpose(p=0.5),\r\n        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\r\n        # 3) Fotométricas\r\n        A.RandomBrightnessContrast(p=0.5),\r\n        # 4) Enmascarado\r\n        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),\r\n    ])\r\n\r\ndef get_validation_augmentation():\r\n    return A.Compose([\r\n        A.Resize(*INPUT_SHAPE[:2]), \r\n    ])\r\n\r\ndef get_preprocessing(preprocessing_fn):\r\n    return A.Compose([\r\n        A.Lambda(image=preprocessing_fn)\r\n    ])\r\n\r\n# ──────────────────────────────────────────────────────────────────────────────\r\n# D) DATASET y DATALOADER para imágenes etiquetadas\r\n# ──────────────────────────────────────────────────────────────────────────────\r\nclass Dataset:\r\n    CLASSES = CLASSES\r\n    def __init__(self, images_dir, masks_dir, classes,\r\n                 augmentation=None, preprocessing=None):\r\n        self.ids        = os.listdir(images_dir)\r\n        self.images_fps = [os.path.join(images_dir, i) for i in self.ids]\r\n        self.masks_fps  = [os.path.join(masks_dir,  i) for i in self.ids]\r\n        self.class_values = [self.CLASSES.index(c.lower()) for c in classes]\r\n        self.augmentation  = augmentation\r\n        self.preprocessing = preprocessing\r\n\r\n    def __getitem__(self, i):\r\n        img_path = self.images_fps[i]\r\n        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\r\n\r\n        mask_path = self.masks_fps[i]\r\n        raw_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\r\n        mask = (raw_mask > 0).astype('float32')[..., None]\r\n\r\n        if self.augmentation:\r\n            augmented = self.augmentation(image=image, mask=mask)\r\n            image, mask = augmented['image'], augmented['mask']\r\n\r\n        if self.preprocessing:\r\n            processed = self.preprocessing(image=image, mask=mask)\r\n            image, mask = processed['image'], processed['mask']\r\n\r\n        return image, mask\r\n\r\n    def __len__(self):\r\n        return len(self.ids)\r\n\r\nclass Dataloder(keras.utils.Sequence):\r\n    def __init__(self, dataset, batch_size=BATCH_SIZE, shuffle=False):\r\n        self.dataset    = dataset\r\n        self.batch_size = batch_size\r\n        self.shuffle    = shuffle\r\n        self.indexes    = np.arange(len(dataset))\r\n        self.on_epoch_end()\r\n\r\n    def __getitem__(self, idx):\r\n        batch_ids = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\r\n        batch = [self.dataset[i] for i in batch_ids]\r\n        return tuple(np.stack(s, axis=0).astype(np.float16) for s in zip(*batch))\r\n\r\n    def __len__(self):\r\n        return int(np.ceil(len(self.dataset) / self.batch_size))\r\n\r\n    def on_epoch_end(self):\r\n        if self.shuffle:\r\n            np.random.seed(SEED)\r\n            np.random.shuffle(self.indexes)\r\n\r\n# ──────────────────────────────────────────────────────────────────────────────\r\n# E) MIXED DATALOADER para Mean Teacher (etiquetados + no-etiquetados)\r\n# ──────────────────────────────────────────────────────────────────────────────\r\n# ──────────────────────────────────────────────────────────────────────────────\r\n# E) MIXED DATALOADER para Mean Teacher (etiquetados + no-etiquetados)\r\n# ──────────────────────────────────────────────────────────────────────────────\r\n# ──────────────────────────────────────────────────────────────────────────────\r\n# E) MIXED DATALOADER para Mean Teacher (etiquetados + no-etiquetados)\r\n# ──────────────────────────────────────────────────────────────────────────────\r\n\r\n\r\nclass MixedDataLoader(keras.utils.Sequence):\r\n    \"\"\"\r\n    Retorna en cada paso: x_lab_batch, y_lab_batch, x_unl_batch.\r\n    Ahora __len__ = número de lotes basados en imágenes etiquetadas,\r\n    y comprueba que las carpetas no estén vacías al inicio.\r\n    \"\"\"\r\n    def __init__(self, x_lab_dir, y_lab_dir, x_unlab_dir,\r\n                 batch_size_lab=BATCH_SIZE, batch_size_unlab=BATCH_SIZE,\r\n                 augment_lab=None, augment_unlab_student=None, augment_unlab_teacher=None, preprocess=None):\r\n\r\n        self.lab_images = sorted(os.listdir(x_lab_dir))\r\n        self.unl_images = sorted(os.listdir(x_unlab_dir))\r\n\r\n        # ── Comprobaciones tempranas ──\r\n        if len(self.lab_images) == 0:\r\n            raise RuntimeError(f\"ERROR: no se encontraron imágenes etiquetadas en:\\n  {x_lab_dir}\")\r\n        if len(self.unl_images) == 0:\r\n            raise RuntimeError(f\"ERROR: no se encontraron imágenes sin etiqueta en:\\n  {x_unlab_dir}\")\r\n\r\n        self.x_lab_dir   = x_lab_dir\r\n        self.y_lab_dir   = y_lab_dir\r\n        self.x_unlab_dir = x_unlab_dir\r\n\r\n        self.batch_size_lab   = batch_size_lab\r\n        self.batch_size_unlab = batch_size_unlab\r\n        self.augment_lab      = augment_lab\r\n        self.augment_unlab_student = augment_unlab_student\r\n        self.augment_unlab_teacher = augment_unlab_teacher\r\n        self.preprocess       = preprocess\r\n\r\n        self.on_epoch_end()\r\n\r\n    def __len__(self):\r\n        # Número de lotes basados únicamente en la parte etiquetada\r\n        n_lab = len(self.lab_images) // self.batch_size_lab\r\n        return max(1, n_lab)\r\n\r\n    def on_epoch_end(self):\r\n        np.random.shuffle(self.lab_images)\r\n        np.random.shuffle(self.unl_images)\r\n\r\n    def __getitem__(self, idx):\r\n        # ◀── Si idx ya no está en [0, len(self)-1], cortamos la iteración\r\n        if idx >= self.__len__():\r\n            raise IndexError\r\n\r\n        # —— Lote etiquetado —— \r\n        start_lab = idx * self.batch_size_lab\r\n        end_lab   = start_lab + self.batch_size_lab\r\n        lab_batch_files = self.lab_images[start_lab:end_lab]\r\n\r\n        x_lab = []\r\n        y_lab = []\r\n        for fname in lab_batch_files:\r\n            img = cv2.cvtColor(\r\n                cv2.imread(os.path.join(self.x_lab_dir, fname)),\r\n                cv2.COLOR_BGR2RGB\r\n            )\r\n            mask = cv2.imread(\r\n                os.path.join(self.y_lab_dir, fname),\r\n                cv2.IMREAD_GRAYSCALE\r\n            )\r\n            mask = (mask > 0).astype('float32')[..., None]\r\n\r\n            if self.augment_lab:\r\n                aug = self.augment_lab(image=img, mask=mask)\r\n                img, mask = aug['image'], aug['mask']\r\n            if self.preprocess:\r\n                pr = self.preprocess(image=img, mask=mask)\r\n                img, mask = pr['image'], pr['mask']\r\n\r\n            x_lab.append(img)\r\n            y_lab.append(mask)\r\n\r\n        x_lab = np.stack(x_lab, axis=0).astype(np.float16)\r\n        y_lab = np.stack(y_lab, axis=0).astype(np.float16)\r\n\r\n        # —— Lote sin-etiquetar —— \r\n        start_unl = idx * self.batch_size_unlab\r\n        end_unl   = start_unl + self.batch_size_unlab\r\n        unl_batch_files = self.unl_images[start_unl:end_unl]\r\n\r\n        x_unl_student, x_unl_teacher = [], []\r\n        \r\n        for fname in unl_batch_files:\r\n            img = cv2.cvtColor(\r\n                cv2.imread(os.path.join(self.x_unlab_dir, fname)),\r\n                cv2.COLOR_BGR2RGB\r\n            )\r\n\r\n            # 1) Debes pasar la imagen RAW (variable `img`), no `img_unl` ni `img_unlab`\r\n            if self.augment_unlab_student:\r\n                aug_s = self.augment_unlab_student(image=img)\r\n                img_unl_student = aug_s['image']\r\n            if self.augment_unlab_teacher:\r\n                aug_t = self.augment_unlab_teacher(image=img)\r\n                img_unl_teacher = aug_t['image']\r\n        \r\n            # 2) Hay que PREPROCESAR ambas versiones (student y teacher), no solo una:\r\n            if self.preprocess:\r\n                img_unl_student = self.preprocess(image=img_unl_student)['image']\r\n                img_unl_teacher = self.preprocess(image=img_unl_teacher)['image']\r\n        \r\n            x_unl_student.append(img_unl_student)\r\n            x_unl_teacher.append(img_unl_teacher)\r\n\r\n        x_unl_student = np.stack(x_unl_student, axis=0).astype(np.float16)\r\n        x_unl_teacher = np.stack(x_unl_teacher, axis=0).astype(np.float16)\r\n        return x_lab, y_lab, x_unl_student, x_unl_teacher\r\n\r\n# ──────────────────────────────────────────────────────────────────────────────\r\n# F) PARSEO DE ARGUMENTOS\r\n# ──────────────────────────────────────────────────────────────────────────────\r\ndef parse_args():\r\n    p = argparse.ArgumentParser()\r\n    p.add_argument(\r\n        \"--mode\",\r\n        choices=[\"supervised\", \"semi\"],\r\n        required=True,\r\n        help=(\r\n            \"supervised → entrenar con model.fit (A-Full y Bxx);\\n\"\r\n            \"semi       → entrenar con Mean Teacher (Cxx).\"\r\n        )\r\n    )\r\n    p.add_argument(\r\n        \"--regime\",\r\n        choices=[\"A-Full\", \"B10\", \"B25\", \"B50\", \"B75\",\r\n                 \"C10\", \"C25\", \"C50\", \"C75\"],\r\n        required=True,\r\n        help=\"Régimen de datos (carpeta dentro de data/all_results/regimes).\"\r\n    )\r\n    return p.parse_args()\r\n\r\n# ──────────────────────────────────────────────────────────────────────────────\r\n# G) FUNCIÓN PARA RAMP-UP DE CONSISTENCY WEIGHT\r\n# ──────────────────────────────────────────────────────────────────────────────\r\ndef get_consistency_weight(epoch):\r\n    if epoch >= CONS_RAMPUP:\r\n        return CONS_MAX\r\n    phase = 1.0 - epoch / CONS_RAMPUP\r\n    return CONS_MAX * np.exp(-5 * phase * phase)\r\n\r\n# ──────────────────────────────────────────────────────────────────────────────\r\n# H) MAIN\r\n# ──────────────────────────────────────────────────────────────────────────────\r\nif __name__ == '__main__':\r\n    args   = parse_args()\r\n    mode   = args.mode      # “supervised” o “semi”\r\n    regime = args.regime    # p.ej. “B25” o “C25”\r\n\r\n    # Definir DATA_DIR según régimen\r\n    DATA_ROOT = r\"C:\\Users\\User\\Desktop\\tesis\\data\\all_results\\regimes\"\r\n    DATA_DIR  = os.path.join(DATA_ROOT, regime)\r\n\r\n    # Rutas de entrenamiento, validación y test\r\n    x_train_lab_dir   = os.path.join(DATA_DIR, 'train', 'images_labeled')\r\n    y_train_lab_dir   = os.path.join(DATA_DIR, 'train', 'masks_labeled')\r\n    x_train_unlab_dir = os.path.join(DATA_DIR, 'train', 'images_unlabeled')\r\n    x_val_dir         = os.path.join(DATA_DIR, 'val',   'images')\r\n    y_val_dir         = os.path.join(DATA_DIR, 'val',   'masks')\r\n    x_test_dir        = os.path.join(DATA_DIR, 'test',  'images')\r\n    y_test_dir        = os.path.join(DATA_DIR, 'test',  'masks')\r\n\r\n    best_iou   = 0.0\r\n    best_epoch = 0\r\n\r\n    # ───── LÓGICA “SUPERVISED” (A-Full y Bxx) ─────\r\n    if mode == \"supervised\":\r\n        best_path = os.path.join(MODEL_DIR, f\"{arch}_best_supervised.weights.h5\")\r\n\r\n        # Crear datasets y loaders para labeled-only\r\n        train_ds = Dataset(\r\n            x_train_lab_dir, y_train_lab_dir, classes=CLASSES,\r\n            augmentation=get_training_augmentation(),\r\n            preprocessing=get_preprocessing(preprocess_input)\r\n        )\r\n        valid_ds = Dataset(\r\n            x_val_dir, y_val_dir, classes=CLASSES,\r\n            augmentation=get_validation_augmentation(),\r\n            preprocessing=get_preprocessing(preprocess_input)\r\n        )\r\n        train_loader = Dataloder(train_ds, batch_size=BATCH_SIZE, shuffle=True)\r\n        valid_loader = Dataloder(valid_ds, batch_size=BATCH_SIZE, shuffle=False)\r\n\r\n        # Crear loader de test\r\n        test_ds    = Dataset(\r\n            x_test_dir, y_test_dir, classes=CLASSES,\r\n            augmentation=get_validation_augmentation(),\r\n            preprocessing=get_preprocessing(preprocess_input)\r\n        )\r\n        test_loader = Dataloder(test_ds, batch_size=BATCH_SIZE, shuffle=False)\r\n\r\n        # Bucle sobre arquitecturas\r\n        for arch in ARCHITECTURES:\r\n            logging.info(f\"Entrenando arquitectura (supervised): {arch}\")\r\n            tf.keras.backend.clear_session()\r\n\r\n            # Construye el modelo, congelando encoder para B-regimes\r\n            freeze_enc = regime.startswith(\"BB\")   # True si es B10/B25/B50/B75\r\n            model = build_model(\r\n                arch, BACKBONE, n_classes, activation, LR,\r\n                input_shape=INPUT_SHAPE,\r\n                freeze_encoder=freeze_enc\r\n            )\r\n            model.summary()\r\n            # ——— LOGGING EXPLÍCITO DE FREEZE ENCODER ———\r\n            # Imprime el flag\r\n            logging.info(f\"freeze_encoder={freeze_enc}\")\r\n            # Cuenta parámetros\r\n            trainable_count     = int(np.sum([K.count_params(w) for w in model.trainable_weights]))\r\n            non_trainable_count = int(np.sum([K.count_params(w) for w in model.non_trainable_weights]))\r\n            logging.info(f\"Trainable params: {trainable_count:,} | Non-trainable params: {non_trainable_count:,}\")\r\n            # ————————————————————————————————————————\r\n\r\n            # Callbacks: checkpoint y reduce_lr + progress monitor\r\n            MODEL_DIR = os.path.join(os.getcwd(), f\"models_supervised_{arch}\")\r\n            os.makedirs(MODEL_DIR, exist_ok=True)\r\n            history_path = os.path.join(MODEL_DIR, f\"{arch}_MT_history_{regime}.csv\")\r\n            best_path = os.path.join(MODEL_DIR, f\"{arch}_best_supervised.weights.h5\")\r\n\r\n            cp = keras.callbacks.ModelCheckpoint(\r\n                filepath=best_path,\r\n                save_weights_only=True,\r\n                save_best_only=True,\r\n                save_freq='epoch',\r\n                verbose=1,\r\n                monitor='val_loss',\r\n                mode='min'\r\n            )\r\n            reduce_lr = keras.callbacks.ReduceLROnPlateau(\r\n                monitor='val_loss', factor=0.5, patience=5, verbose=1\r\n            )\r\n            monitor_cb = ProgressMonitor(batch_print_freq=25)\r\n\r\n            # Entrenar\r\n            model.fit(\r\n                train_loader,\r\n                steps_per_epoch=len(train_loader),\r\n                epochs=EPOCHS,\r\n                callbacks=[cp, reduce_lr, monitor_cb],\r\n                validation_data=valid_loader,\r\n                validation_steps=len(valid_loader)\r\n            )\r\n\r\n            # Cargar pesos antes de evaluar test\r\n            model.load_weights(best_path)\r\n\r\n\r\n            # Evaluar\r\n            val_metrics  = model.evaluate(valid_loader, verbose=0)\r\n            test_metrics = model.evaluate(test_loader,  verbose=0)\r\n\r\n            logging.info(\r\n                f\"{arch} (supervised): \"\r\n                f\"Val Loss={val_metrics[0]:.4f}, Val IoU={val_metrics[1]:.4f} | \"\r\n                f\"Test Loss={test_metrics[0]:.4f}, Test IoU={test_metrics[1]:.4f}\"\r\n            )\r\n\r\n    # ───── LÓGICA “SEMI” (Mean Teacher para Cxx) ─────\r\n    elif mode == \"semi\":\r\n        # MixedDataLoader para train (labeled + unlabeled)\r\n        # Definir las augmentaciones aquí (antes de usar el DataLoader)\r\n        strong_aug = get_strong_augmentation(INPUT_SHAPE)\r\n        weak_aug   = get_weak_augmentation(INPUT_SHAPE)\r\n\r\n        # Crear el DataLoader con la augmentación diferente para el estudiante y el profesor\r\n        train_mixed_loader = MixedDataLoader(\r\n            x_lab_dir=x_train_lab_dir,\r\n            y_lab_dir=y_train_lab_dir,\r\n            x_unlab_dir=x_train_unlab_dir,\r\n            batch_size_lab=BATCH_SIZE,\r\n            batch_size_unlab=BATCH_SIZE,\r\n            augment_lab=strong_aug,  \r\n            augment_unlab_student=strong_aug,\r\n            augment_unlab_teacher=weak_aug, \r\n            preprocess=get_preprocessing(preprocess_input)\r\n        )\r\n\r\n        # Validación y test: student usa Dataset/Dataloder normales\r\n        val_loader = Dataloder(\r\n            Dataset(x_val_dir, y_val_dir, classes=CLASSES,\r\n                    augmentation=get_validation_augmentation(),\r\n                    preprocessing=get_preprocessing(preprocess_input)),\r\n            batch_size=BATCH_SIZE, shuffle=False\r\n        )\r\n        test_loader = Dataloder(\r\n            Dataset(x_test_dir, y_test_dir, classes=CLASSES,\r\n                    augmentation=get_validation_augmentation(),\r\n                    preprocessing=get_preprocessing(preprocess_input)),\r\n            batch_size=BATCH_SIZE, shuffle=False\r\n        )\r\n\r\n        # Entrenar para cada arquitectura\r\n        for arch in ARCHITECTURES:\r\n            logging.info(f\"Entrenando arquitectura (Mean Teacher): {arch}\")\r\n            tf.keras.backend.clear_session()\r\n\r\n            # Construir student y teacher\r\n            student = build_model(arch, BACKBONE, n_classes, activation, LR,\r\n                                  input_shape=INPUT_SHAPE)\r\n            teacher  = build_model(arch, BACKBONE, n_classes, activation, LR,\r\n                                   input_shape=INPUT_SHAPE)\r\n            teacher.set_weights(student.get_weights())\r\n            teacher.trainable = False\r\n\r\n            base_optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\r\n            optimizer      = mixed_precision.LossScaleOptimizer(base_optimizer)\r\n\r\n            dice_loss = sm.losses.DiceLoss()\r\n            mse_loss  = tf.keras.losses.MeanSquaredError()\r\n\r\n            MODEL_DIR = os.path.join(os.getcwd(), f\"models_MT_{arch}\")\r\n            os.makedirs(MODEL_DIR, exist_ok=True)\r\n            history_path = os.path.join(MODEL_DIR, f\"{arch}_MT_history_{regime}.csv\")\r\n            best_iou   = 0.0\r\n            best_epoch = 0\r\n            best_path  = os.path.join(MODEL_DIR, f\"{arch}_best_student.weights.h5\")\r\n\r\n            with open(history_path, 'w', newline='', encoding='utf-8') as f:\r\n                writer = csv.writer(f)\r\n                # Training loop manual\r\n\r\n\r\n                for epoch in range(EPOCHS):\r\n                    logging.info(f\"--- Época {epoch+1}/{EPOCHS} (Mean Teacher) ---\")\r\n                    cons_w = get_consistency_weight(epoch)\r\n\r\n                    for x_lab, y_lab, x_unl_s, x_unl_t in train_mixed_loader:\r\n                        # → Teacher sobre versión débil\r\n                        t_preds = teacher(x_unl_t, training=False)\r\n                        t_probs = tf.sigmoid(t_preds)\r\n\r\n                        with tf.GradientTape() as tape:\r\n                            # Student en labeled\r\n                            s_lab_preds = student(x_lab, training=True)\r\n                            s_lab_probs = tf.sigmoid(s_lab_preds)\r\n                            loss_sup  = dice_loss(y_lab, s_lab_probs)\r\n\r\n                            # DEBUG:\r\n                            logging.info(f\"DEBUG dtype y_lab: {y_lab.dtype}, dtype s_lab_probs: {s_lab_probs.dtype}\")\r\n                            logging.inf\r\n\r\n                            # Student en unlabeled\r\n                            s_unl_preds = student(x_unl_s, training=True)\r\n                            s_unl_probs = tf.sigmoid(s_unl_preds)\r\n                            loss_cons = mse_loss(t_probs, s_unl_probs)\r\n\r\n                            loss_total = loss_sup + UNLABELED_WEIGHT * cons_w * loss_cons\r\n                    \r\n                            scaled_loss = optimizer.get_scaled_loss(loss_total)\r\n\r\n\r\n                        scaled_grads = tape.gradient(scaled_loss, student.trainable_variables)\r\n\r\n                        grads        = optimizer.get_unscaled_gradients(scaled_grads)\r\n\r\n                        optimizer.apply_gradients(zip(grads, student.trainable_variables))\r\n\r\n                        # Actualizar teacher con EMA\r\n                        sw = student.get_weights()\r\n                        tw = teacher.get_weights()\r\n                        new_tw = [\r\n                            EMA_ALPHA * tw_i + (1.0 - EMA_ALPHA) * sw_i\r\n                            for sw_i, tw_i in zip(sw, tw)\r\n                        ]\r\n                        teacher.set_weights(new_tw)\r\n\r\n                    # ────────────────────────────────────────────────────────────\r\n                    # ▼ EN LUGAR de recorrer val_loader manualmente, llamamos a evaluate_and_log\r\n                    try:\r\n                        val_loss, val_iou, val_prec, val_rec, val_f1 = evaluate_and_log(\r\n                            student,      # tu modelo student\r\n                            val_loader,   # tu DataLoader de validación\r\n                            writer,       # CSV writer\r\n                            epoch,        # número de época\r\n                            loss_sup,     # pérdida supervisada calculada en el batch\r\n                            loss_cons,    # pérdida de consistencia\r\n                            loss_total    # suma de las dos anteriores\r\n                        )                        \r\n                        logging.info(\r\n                            f\"Época {epoch}: sup={loss_sup:.4f}, cons={loss_cons:.4f}, total={loss_total:.4f} | \"\r\n                            f\"val_loss={val_loss:.4f}, IoU={val_iou:.4f}, P={val_prec:.4f}, \"\r\n                            f\"R={val_rec:.4f}, F1={val_f1:.4f}\"\r\n                        )\r\n\r\n                    except Exception as e:\r\n                        # Alerta si evaluate_and_log falla por algún motivo\r\n                        print(f\"[main_semisup_g] ERROR en evaluate_and_log en época {epoch+1}: {e}\")\r\n                        val_loss, val_iou = float('nan'), float('nan')\r\n\r\n                    logging.info(\r\n                        f\"{arch} (MT) Época {epoch+1}: Val Loss={val_loss:.4f}, Val IoU={val_iou:.4f}\"\r\n                    )\r\n                    # ────\r\n                    # ─────────────────────────────────────────────\r\n                    # Guardar sólo si esta época supera el mejor IoU_val\r\n                    if val_iou > best_iou:\r\n                        best_iou   = val_iou\r\n                        best_epoch = epoch + 1\r\n                        student.save_weights(best_path)\r\n                        logging.info(f\"[semi] ▶ Nuevo best IoU_val={best_iou:.4f} (época {best_epoch})\")\r\n\r\n                # Evaluación final en test (student)\r\n# ─────────────────────────────────────────────\r\n                # Carga el mejor modelo según validación\r\n                student.load_weights(best_path)\r\n                logging.info(f\"[semi] ✔ Cargado best checkpoint época {best_epoch} con IoU_val={best_iou:.4f}\")\r\n\r\n                test_metrics = student.evaluate(test_loader, verbose=0)\r\n                test_loss, test_iou = test_metrics[0], test_metrics[1]\r\n                logging.info(\r\n                    f\"{arch} (MT) Test final: Loss={test_metrics[0]:.4f}, IoU={test_metrics[1]:.4f}\"\r\n                )\r\n                       \r\n\r\n    else:\r\n        raise ValueError(\"Modo no reconocido: use --mode supervised o --mode semi\")\r\n\r\n    print(\"¡Entrenamiento completado!\")\r\n"
        }
    ]
}